# Hotplace Recommand<br>

### 개요

  해당 프로그램은 해쉬태그 및 공공데이터에 근거하여 사용자에게 핫플레이스(여행장소)를 추천해주는 프로그램입니다. 

  최근, 이태원 압사사고 등으로 인하여 국민들이 안전에 대해 높은 관심을 가지고 있습니다. 그럼에도 불구하고 저희 팀은 네이버 플레이스, 인스타그램 등 일반적으로 자주 사용하는 여행장소 검색 프로그램은 안전에 대한 정보를 아예 제공하지 않거나, 혹은 제한적으로 제공하는 등, 이러한 시대적 흐름에 따르지 못하고 있는 것을 확인할 수 있었습니다. 그렇기에 저희는 공공데이터API로부터 안전 사고의 종류 혹은 횟수에 대한 데이터를 받고, 이를 근거로 웹 스크래핑(크롤링)을 진행하여, 안전과 여행장소에 대한 정보를 유저들에게 제공하고자 합니다.

  더 나아가, 기존 프로그램들은 신뢰성 부분에서 한계를 지니고 있습니다. 예컨대 높은 조회수를 위하여 해쉬태그를 남발하거나, 혹은 광고를 위하여 편향된 글을 작성하는 것이 한계라고 볼 수 있겠습니다. 그렇기에 저희는 별도의 신뢰도 평가 프로그램을 계획하여, 과연 해당 리뷰(게시글)이 사용자에게 추천할 수 있을 정도로 신뢰도가 높은지를 별도로 평가하여, 제공할 것입니다.

  이처럼 저희팀은, **안전 정보에 대한 부재**, **신뢰도 부족** 이라는 기존 프로그램의 한계를 해결하여 진실로 필요한 프로그램을 계획하고자 하였습니다.


#### 저장소 관리 -> 김도겸
#### DFD 제작 -> 박지원, 박채림
#### PPT 제작 -> 김도겸, 박종범
#### 발표 -> 박종범
#### 필요 자료 조사 -> 김민규, 김기범
#### 오픈 소스 조사 -> 모든 팀원


![DFD](https://user-images.githubusercontent.com/85345952/205487119-bb137b78-b6e0-4214-91ec-66c3888fcddc.png)


Kospeech<br><br>
=============
<br>종단 간 음성 인식 모델을 개발하기 위해 PyTorch에 구축된 Apache 2.0 ASR 연구 라이브러리

<hr>

 한국어 자동 음성 인식 기능<br>

오픈 소스 소프트웨어인 KoSpeech 는 딥 러닝 라이브러리인 PyTorch를 기반으로 하는 모듈식 확장형 한국어 자동 음성 인식(ASR) 툴킷입니다. 여러 자동 음성 인식 오픈 소스 툴킷이 출시되었지만 모두 영어와 같은 비한국어를 처리합니다(예: ESPnet, Espresso). 
<br>
<br>
AI Hub는 KsponSpeech로 알려진 한국어 음성 코퍼스 1,000시간을 공개했지만 모델 성능을 비교할 수 있는 전처리 방법과 기준 모델이 확립되어 있지 않습니다. <br>
<br>
따라서 KsponSpeech 말뭉치에 대한 전처리 방법과 여러 모델(Deep Speech 2, LAS, Transformer, Jasper, Conformer)을 제안합니다.<br>
<br><br>


End to End 모델 사용
-----------
한국어의 STT 난이도가 높은 이유는 많은 자음과 모음의 조합뿐 아니라 음소, 음절 등 고려해야 하는 사항들이 너무 많다는 것입니다. <br>
그런데, 2012년 이후 딥러닝이 발전하면서 고려해야 하는 사항들이나 다양한 변수들에 대한 처리를 layer에 맡기는 것이 가능해졌습다. <br>
따라서 음소나 음절의 처리, 문법, 발음 등을 모두 학습하게 하는 End to End 모델이 가능해졌으며 해당 서비스에 적용할 kospeech 또한 End to End 모델을 사용하여 처리합니다. <br>
요약하면, End to End 모델은 음성 데이터가 포함하는 문법, 발음 등 여러 특징을 모두 모델이 학습하도록 하여 input으로 Raw Audio를 통째로 넣을 수 있도록 한 것입니다. <br><br><br>

전처리 지원
---------

STT를 구현하기 위해서 음성 데이터와, 이를 전사를 한 Label이 필요합니다. <br>
전사는 말소리를 음성 문자로옮겨 적는 것이다. 더 자세하게 어, 그, 음 와 같은 간투어, 주변 소음 등에 따라 녹음된 내용을 사람이 기록해 놓은 것을 말합니다.<br>
사용자는 데이터의 특징에 따라 전사규칙을 삭제하고 문장 부호를 삭제하는 등의 전처리를 진행해야 했으나 kospeech는 개발자들이 사용한 ai-hub의 데이터(kspon)와 libri에 한해 전처리 과정을 공개했습니다.<br>
<br><br><br>


파이프 라인
------------
Raw audio을 입력하면 학습된 feature transform을 거치도록 합니다. <br>
Acoustic model에 추출한 특징들이 전달되고 해당 모델이 이 특징을 통해 발화 예상을 가능하게 한다. 특징들이 모델과 CTC 알고리즘 통과하며 텍스트로 출력됩니다.<br>
CTC(Connectionist Temporal Classification)는 신호와 텍스트 사이의 alignment를 알기 어려움을 해소하기 위해 적용하는 알고리즘입니다. CTC기반의 음성인식은 초기에 제안된 간단한 형태로 학습 과정에서 음성이 인코더를 통과한 결과에 소프트맥스(Softmax)를 취하고 여기에서 CTC 손실을 계산한 뒤 이를 줄이도록 학습합니다.
<br><br><br>


모듈 설치
-----------
```!pip install -r requirements_cssiri.txt```
<br>
- Python 3.8을 사용했습니다.<br>
- kospeech가 제공하는 다양한 Acoustic Model 중, ds2(deepspeech2)를 사용했습니다.<br>
- Pytorch의 경우 1.10 버전이 사용되기 때문에 상위 버전을 사용하시는 경우 별도로 Pytorch를 재설치해주어야 합니다.<br>
- 전처리, 학습, 예측, 예측한 결과 저장에 필요한 모든 모듈을 포함시켰습니다.<br><br><br><br>

전처리(Preprocess)
------------
```!python ./dataset/kspon/main.py --dataset_path $dataset_path --vocab_dest $vacab_dict_destination ---- -- output_unit 'character' --preprocess_mode 'phonetic' ```
<br>
- output_unit과 preprocess_mode는 상황에 맞게 지정해주시면 됩니다.<br>
- ./dataset/kspon/preprocess/preprocess.py의 line 95~101을 확인해보시면, './'의 위치에 'train.txt' 파일을 필요로 합니다. 해당 파일은 '음성 파일 경로' + '\t' + '한국어 전사' 의 형식으로 작성되어야 합니다.<br>
train.txt를 만들 때 사용한 코드는 ./etc/traintext 생성.ipynb 에 올려져 있습니다.<br><br><br>


학습(Train)
-------------------
```!python ./bin/main.py model=ds2 train=ds2_train train.dataset_path=$dataset_path```
<br>
- 학습과 관련된 configs(epoch, batch_size, spec_augment, 음성 파일 확장자 등)의 수정은 ./configs/audio/fbank.yaml 혹은 ./configs/train/ds2_train.yaml 에서 하실 수 있습니다.<br><br><br><br>


kospeech 라이선스
--------------
- Apache License 2.0<br>
GPL과는 달리 소스 코드 공개의 의무가 없고, 2차 라이선스와 변형물의 특허 출원이 가능합니다. <br>
아파치 라이선스 (2.0 기준)은 누구나 해당 소프트웨어에서 파생된 프로그램을 제작할 수 있으며 저작권을 양도, 전송할 수 있는 라이선스 규정을 의미합니다. 
아파치 라이선스에 따르면 누구든 자유롭게 아파치 소프트웨어를 다운 받아 부분 혹은 전체를 개인적 혹은 상업적 목적으로 이용할 수 있으므로 충돌이나 문제가 발생하지 않을 것으로 예상합니다.<br>
<br>

References <br>
Wer, Cer 관련: https://holianh.github.io/portfolio/Cach-tinh-WER/ <br>
kospeech: https://github.com/sooftware/kospeech
<br><br><br><br>

elasticsearch
==========

<br>엘라스틱 서치는 검색 엔진입니다. 여기서 검색 엔진(search engine)이란, 웹에서 정보를 수집하여 검색 결과를 제공하는 프로그램입니다. <br>
<hr>
Elasticsearch는 Elastic Stack 의 핵심인 분산형 RESTful 검색 및 분석 엔진입니다. <br>
Elasticsearch를 사용하여 다음에 대한 데이터를 저장, 검색 및 관리할 수 있습니다. <br>
- 로그<br>
- 측정항목<br>
- 검색 백엔드<br>
- 애플리케이션 모니터링<br>
- 엔드포인트 보안<br><br>
엘라스틱서치에서는 비정형 데이터를 색인하고 검색하는 것이 가능하며 역색인 구조을 사용함으로써 빠른 검색이 가능합니다.<br>
Elasticsearch를 설정하는 가장 간단한 방법은 Elastic Cloud에서 Elasticsearch Service 로 관리형 배포를 생성하는 것 입니다.<br>
Elasticsearch를 직접 설치하고 관리하려면 https://www.elastic.co/kr/downloads/elasticsearch 에서 최신 버전을 다운로드할 수 있습니다.<br>
<br>

데이터 추가
---------
REST API를 통해 JSON 개체(문서)를 전송하여 데이터를 Elasticsearch로 인덱싱합니다. <br>
구조화된 텍스트, 구조화되지 않은 텍스트, 숫자 데이터 또는 지리 공간 데이터가 있든 관계없이 Elasticsearch는 빠른 검색을 지원하는 방식으로 이를 효율적으로 저장하고 인덱싱합니다.
<br><br>

오픈소스 검색엔진
---------

<br>엘라스틱 서치는 아파치 재단의 루씬을 기반으로 개발된 오픈소스 검색엔진입니다.<br><br>

전문 검색 
---------

<br>전문 검색이란, 내용 전체를 색인해서 특정 단어가 포함된 문서를 검색하는 것이다. 엘라스틱서치는 이러한 전문 검색이 가능합니다.<br><br>

통계분석
----------

<br>비정형 로그 데이터를 수집하고 한곳에 모아 통계 분석을 할 수 있습니다.<br><br>

역색인
------------

<br>역색인 구조를 통해 특정 단어를 찾을 때 문서 전체에서 찾는 것이 아니라 단어가 포함된 특정 문서의 위치를 알아내어 빠르게 결과를 찾아낼 수 있습니다.<br><br>

라이선스
------------

- Elastic License 2.0 
<br>Elastic 2.0는 오픈소스 라이선스의 거의 모든 자유를 허용하며 소프트웨어 수신자는 소프트웨어를 자유롭게 사용, 변경 및 재배포 할 수 있습니다.

# HotplaceRecommand

# Scrapy

작성자 : 1971352 - 김기범



+ Scrapy를 고르게된 이유
+ Scrapy의 라이선스
+ Scrapy란
+ Scrapy의 특징
+ 크롤링과 스크래핑
+ Scrapy 프로그램 설계 시 고려해야 할 내용
+ 타 프로그램과의 상호작용



#### Scrapy를 고르게된 이유

해시태그기반 핫플추천이라는 주제에 대해 생각해보니 우선, 그 사이트를 분석해주는 오픈소스가 필요하다는 것을 생각하게되었고 그 과정에서 찾은 크롤링 오픈소스는 2가지였습니다. 

	하나는 Selenium 또 다른 하나는 Scrapy입니다. 셀레니움은 페이지를 렌더링 하기 위해 필요한 js, css 그리고 image 파일까지 불러오며 속도가 많이 지연된다. 하지만 scrapy는 지정된 url만 조회합니다. 그렇기 때문에 scrapy가 셀레니움보다 가볍고 빠른 퍼포먼스를 낼 수 있는것이 첫번째 이유입니다. 

	인스타그램을 크롤링 할때에는 주식자료처럼 동적이 아닌 정적인 자료이다 보니 scrapy가 좀더 효율적이라고 판단 하였습니다. 

	우려했던 부분인 Scrapy는 lxml을 기반으로 만들어져 있어서 자바스크립트를 읽고 실행하지 못한다 라는 단점은 인스타그램이 파이썬기반인 것을 알게 되어서 괜찮다고 생각하였습니다. (자바스크립트를 처리하기 위해 Selenium과PhantomJS를 주로 사용하는데, Scrapy에서도 Splash를 사용하면 자바스크립트를 처리할 수 있다.) 마지막으로 Django 와 같은 백엔드 서비스와 연동하기도 좋고, 플러그인도 다양해서 여러모로 만능인 프레임워크이기에 선택하게되었습니다.



#### Scrapy란 

**스크래피**(Scrapy)는 Python으로 작성된 오픈소스 웹 크롤링 프레임워크입니다. 웹 데이터를 수집하는 것을 목표로 설계되었습니다. 또한 API를 이용하여 데이터를 추출할 수 있고, 범용 웹 크롤러로 사용될 수 있습니다.

Scrapy 프로젝트는 "spiders"를 중심으로 개발되었습니다. "spiders"는 여러 기능이 내장된 크롤러이다. 장고와 같은 철학인 중복배제를 따르고 있는 프레임워크입니다. Scrapy는 개발자들이 코드 재사용성을 높일 수 있도록 도와주어, 큰 규모의 크롤링 프로젝트 개발을 쉽게 할 수 있도록 해줍니다. 또한 Scrapy는 개발자들이 크롤링하려는 사이트의 동작을 테스트할 수 있도록 웹 크롤링 셸을 제공합니다.



#### Scrapy의 특징

- 비동기 네트워킹 라이브러리 Twisted 기반
- selenium 처럼 Xpath, CSS표현식으로 HTML소스에서 데이터 추출이 가능
- **webdriver 사용하지 않아 빠른 퍼포먼스를 기대할수있다.**
- 다운로드 타임아웃, request 사이의 random한 텀 두기 등 다양한 옵션
- scrapy를 사용하기 위해서는 scrapy 기반 프로젝트를 생성
- **spider를 작성**해 크롤링
- javascript지원 힘들다.
- ajax/pjax로 데이터가 갱신되는 웹페이지라면 원하는 데이터를 추출하는 게 어렵다.



#### Scrapy의 라이선스

해당 오픈소스는 **BSD 3-Clause "New" or "Revised"** 라이선스에 기반하고 있습니다. BSD 2-Clause 라이선스인  유사하지만 다른 사람이 서면 동의 없이 파생 제품을 홍보하기 위해 저작권 보유자 또는 기여자의 이름을 사용하는 것을 금지하는 3번째 조항이 포함된 허용 라이선스입니다.

BSD 2-Clause

- 저작권자 표기. BSD 라이선스가 걸린 소프트웨어를 이용하는 경우, 해당 저작권자의 이름과 BSD 라이선스의 내용을 같이 배포해야 한다.
- 보증 부인. BSD 라이선스가 걸린 소프트웨어로 인하여 법률상/도의상 피해가 발생하더라도, 그 책임을 저작권자에게 물을 수 없다.



#### 웹 크롤링과 웹 스크래핑

크롤링과 스크래핑이 혼용되어 서로 바꾸어 사용하는 경향이 있고 서로 밀접하게 관련되어 있지만, 두 프로세스 사이에 차이점이 있기때문에 설명드리고자 합니다.

일반적으로 크롤러라는 용어는 명확한 최종 목표나 목표가 없어도 사이트나 네트워크가 제공할 수 있는 것을 끝없이 탐색하면서 스스로 웹 페이지를 탐색할 수 있는 프로그램의 능력을 의미합니다. 웹 크롤러는 Google, Bing 등과 같은 검색 엔진에서 URL의 콘텐츠를 추출하고, 이 페이지에서 다른 링크를 확인하고, 링크의 URL을 가져오는 데 주로 사용됩니다.

반면에 웹 스크래퍼는 특정 데이터를 추출하는 프로세스입니다.

웹 크롤링과 달리 웹 스크래퍼는 특정 웹 사이트 또는 페이지에서 특정 정보를 검색합니다. 기본적으로 웹 크롤링은 기존의 복사본을 만들고, 웹 스크래핑은 분석을 위한 특정 데이터를 추출하거나 새로운 것을 만듭니다. 그러나 웹 스크래핑을 수행하기 위해서는 먼저 필요한 정보를 찾기 위해 웹 크롤링 같은 작업을 수행해야 합니다.

데이터 크롤링에는 웹 페이지의 모든 키워드, 이미지 및 URL을 저장하는 것과 같은 특정 수준의 스크래핑이 필요합니다.



예시를 들어보자면 웹 크롤링은 일반적으로 Google, Yahoo, Bing등이 정보를 검색하는 방식입니다. 반면에, 웹 스크래핑은 주식 시장과 같이 특정 데이터에 대한 특정 웹 사이트를 대상으로 하는 것입니다.



우리가 설계한 프로그램같은경우는 특정한 사이트에 국한되지않도록 계획하기때문에 크롤링이 맞다고 생각됩니다.



#### Scrapy 프로그램 설계 시 고려해야 할 내용

`Scrapy`는 프레임 워크이기때문에 주어진 규칙에 따라 프로젝트를 진행해야 합니다. 그 과정속에서 크게 `Spider`와 `Item`, `Item pipeline`이 3가지로 진행됩니다. 

`Spider`는 어떤 웹 사이트들을 어떠한 규칙에 따라 크롤링할 것인지 명시하고, 각각의 웹 페이지의 어떤부분을 스크래핑할 것인지 등을 일괄적으로 명시하는 클래스입니다. 즉, 웹크롤링과 스크래핑 규칙을 설정 하는 핵심요소입니다. 여기서 좀더 자세히 나가자면 웹페이지를 가지고 올때, 일반적으로 수행해야 할 작업은 HTML소스에서 데이터를 추출하는 것입니다. 이를 위해 Scrapy에는 데이터 추출을 위한 자체 매커니즘이 있습니다. XPath또는 CSS표현식으로 지정 된 HTML 문서의 특정 부분을 선택하기 때문에 Selector라고 합니다.

`Item`은 Scrapy에서 기본 제공하는 자료구조 클래스입니다. 새로운 Item 클래스를 정의하고 여기에 우리가 수집하고자 하는 정보들을 명시하면, Spider 상에서 실제 스크랭핑을 수행환 결과물을, 파일형태로 저장할 때, item들을 어떻게 처리할 것인지 명시하면, 해당 규칙에 따라 데이터를 가공하거나 혹은 외부 파일로 간편하게 저장할 수 있습니다.

`Item pipeline`는 클래스를 새로 정의하고 여기에 각 Item들을 어떻게 처리할 것인지 명시하면, 해당 규칙에 따라 데이터를 가공하거나 혹은 외부 파일로 간편하게 저장할 수 있습니다.

진행과정은 다음과 같습니다

1. 새로운 Scrapy 프로젝트 생성

2. Item 정의

3. Spider 작성

   3-1 Selectors 설정 

4. Item pipeline



**이 과정 속에서 구체적으로 생각해야 되는것은**

1. **Item및 Item pipeline구성에 대해 고민해 볼 필요가 있었습니다.** 크롤링시, 불필요한 데이터도 있기때문에 무슨 데이터가 필요하며 그것을 어떤식으로 처리할지에 대해 고민하였습니다.
2. **크롤링 범위에 대해 고민할 필요가 있었습니다.**  모든 사이트를 크롤링 하겠다라는 것은 너무 광범위하고 허황된 이야기 인것같아서 시스템 구상 단계에서 생각한 범위는 우선 공공데이터 오픈 API 를 통해 여행 장소에 대한 자료를 제공받습니다. 주로 각 시도 기관에서 여행관련된 CSV , JSON 형식의 파일을 오픈 API를 통해 제공하고 있습니다. 이를 Scrapy의 정보 수집 기준점으로 삼아, 보다 정확한 여행 자료를 수집할 수 있도록 돕고자 합니다.
   뿐만 아니라 공공데이터 오픈 API를 통하여 사건사고 데이터를 받고 이를 Scrapy 에게 수집 기준점 으로 제시하여 사건 사고와 관련된 자료를 크롤링하고자 합니다.
3. **크롤링시 허용되는 범위에 대해 고민해 볼 필요가 있었습니다.**  무분별한 크롤링시 웹사이트의 이용약관을 위반하거나 다른 사람의 사생활을 침해할 수도있다는 글을 본뒤로 크롤링시 허용되는 범위에 대해 조사할 필요성을 느꼈습니다. 그에 따라 조사하던중 robots.txt를 사용한다면 이러한 문제를 예방할 수 있다고 합니다.



#### 타 프로그램과의 상호작용

웹사이트 크롤링(scrapy)>정규표현식으로 파싱한 후, 내용 분석(khaiii)->분석된 내용과 url주소등을 Django에게 보냄->데이터베이스에 저장(Django)

이것이 현재 제 파트에서 이루어지는 과정입니다. 따라서 첫번째 과정에서 검색한 키워드를 받게되면 Scrpay를 통해 웹크롤링을 진행하여 데이터를 얻습니다. ex) 공공데이터 오픈 API

 그후 이 데이터를 정규 표현식으로파싱합니다. 이 파싱한 내용을 형태소 분석기를 통해 내용을 분석하는 과정을 거칩니다. 그후 분석된 자료(해시태그, 게시글)와 url주소를 데이터베이스에 저장합니다. 또한 데이터를 호출하거나 저장할때 Django와의 연계할수있는 기능들이 있다고하여 보다 쉽게 데이터를 처리할수있다고 예상됩니다.



#### 참조

작성자는 아래 URL을 참고, 인용하여 해당 문서를 작성하였습니다.

Scrapy 깃허브 주소

https://github.com/scrapy/scrapy

Scrapy 위키백과

https://ko.wikipedia.org/wiki/%EC%8A%A4%ED%81%AC%EB%9E%98%ED%94%BC_(%EC%9B%B9_%ED%94%84%EB%A0%88%EC%9E%84%EC%9B%8C%ED%81%AC)

scrapy 소개

https://jybaek.tistory.com/927

Scrapy 기본구조와 간단한 웹 스크래핑

https://nittaku.tistory.com/135

정의 및 Django와의 연계활용예시

https://blog.theodo.com/2019/01/data-scraping-scrapy-django-integration/

셀렉터 정의및 예시

https://overit.tistory.com/entry/Scrapy-Selector

크롤링 활용예시

[https://pycoding.tistory.com/entry/scrapy%EC%8A%A4%ED%81%AC%EB%9E%98%ED%94%BC-%ED%81%AC%EB%A1%A4%EB%A7%81-%EC%8B%A4%EC%A0%84-csv-%EC%A0%80%EC%9E%A5%EA%B9%8C%EC%A7%80](https://pycoding.tistory.com/entry/scrapy스크래피-크롤링-실전-csv-저장까지)

 https://hansuho113.tistory.com/7 

**[Django] Celery,Redis로 Scrapy 크롤링 주기적으로 하기 (feat. Docker)**

 ( https://qwlake.github.io/django/2020/05/15/django-scrapy-with-celery-redis/ )

# khaiii

khaiii는 "Kakao Hangul Analyzer III"의 첫 글자들만 모아 만든 이름으로 카카오에서 개발한 세 번째 형태소분석기입니다. 두 번째 버전의 형태소분석기 이름인 dha2 (Daumkakao Hangul Analyzer 2)를 계승한 이름이기도 합니다.

형태소는 언어학에서 일정한 의미가 있는 가장 작은 말의 단위로 발화체 내에서 따로 떼어낼 수 있는 것을 말합니다. 즉, 더 분석하면 뜻이 없어지는 말의 단위입니다. 형태소분석기는 단어를 보고 형태소 단위로 분리해내는 소프트웨어를 말합니다. 이러한 형태소분석은 자연어 처리의 가장 기초적인 절차로 이후 구문 분석이나 의미 분석으로 나아가기 위해 가장 먼저 이루어져야 하는 과정으로 볼 수 있습니다.

## 데이터 기반

기존 버전이 사전과 규칙에 기반해 분석을 하는 데 반해 khaiii는 데이터(혹은 기계학습) 기반의 알고리즘을 이용하여 분석을 합니다. 학습에 사용한 코퍼스는 국립국어원에서 배포한 [21세기 세종계획 최종 성과물](https://ithub.korean.go.kr/user/noticeView.do?boardSeq=1&articleSeq=16)을 카카오에서 오류를 수정하고 내용을 일부 추가하기도 한 것입니다. 전처리 과정에서 오류가 발생하는 문장을 제외하고 약 85만 문장, 천만 어절의 코퍼스를 사용하여 학습을 했습니다.

## 알고리즘

기계학습에 사용한 알고리즘은 신경망 알고리즘들 중에서 Convolutional Neural Network(CNN)을 사용하였습니다. 한국어에서 형태소분석은 자연어처리를 위한 가장 기본적인 전처리 과정이므로 속도가 매우 중요한 요소라고 생각합니다. 따라서 자연어처리에 많이 사용하는 Long-Short Term Memory(LSTM)와 같은 Recurrent Neural Network(RNN) 알고리즘은 속도 면에서 활용도가 떨어질 것으로 예상하여 고려 대상에서 제외하였습니다.

## 성능

### 정확도

#### v0.3

CNN 모델의 주요 하이퍼 파라미터는 분류하려는 음절의 좌/우 문맥의 크기를 나타내는 win 값과, 음절 임베딩의 차원을 나타내는 emb 값입니다. win 값은 {2, 3, 4, 5, 7, 10}의 값을 가지며, emb 값은 {20, 30, 40, 50, 70, 100, 150, 200, 300, 500}의 값을 가집니다. 따라서 이 두 가지 값의 조합은 6 x 10으로 총 60가지를 실험하였고 아래와 같은 성능을 보였습니다. 성능 지표는 정확률과 재현율의 조화 평균값인 F-Score입니다.

win 파라미터의 경우 3 혹은 4에서 가장 좋은 성능을 보이며 그 이상에서는 오히려 성능이 떨어집니다. emb 파라미터의 경우 150까지는 성능도 같이 높아지다가 그 이상에서는 별 차이가 없습니다. 최 상위 5위 중 비교적 작은 모델은 win=3, emb=150으로 F-Score 값은 97.11입니다. 이 모델을 large 모델이라 명명합니다.

#### v0.4

[띄어쓰기 오류에 강건한 모델을 위한 실험](https://github.com/kakao/khaiii/wiki/띄어쓰기-오류에-강건한-모델을-위한-실험)을 통해 모델을 개선하였습니다. v0.4 모델은 띄어쓰기가 잘 되어있지 않은 입력에 대해 보다 좋은 성능을 보이는데 반해 세종 코퍼스에서는 다소 정확도가 떨어집니다. 이러한 점을 보완하기 위해 base 및 large 모델의 파라미터를 아래와 같이 조금 변경했습니다.

- base 모델: win=4, emb=35, F-Score: 94.96
- large 모델: win=4, emb=180, F-Score: 96.71

### 속도

#### v0.3

모델의 크기가 커지면 정확도가 높아지긴 하지만 그만큼 계산량 또한 많아져 속도가 떨어집니다. 그래서 적당한 정확도를 갖는 모델 중에서 크기가 작아 속도가 빠른 모델을 base 모델로 선정하였습니다. F-Score 값이 95 이상이면서 모델의 크기가 작은 모델은 win=3, emb=30이며 F-Score는 95.30입니다.

속도를 비교하기 위해 1만 문장(총 903KB, 문장 평균 91)의 텍스트를 분석해 비교했습니다. base 모델의 경우 약 10.5초, large 모델의 경우 약 78.8초가 걸립니다.

#### v0.4

모델의 크기가 커짐에 따라 아래와 같이 base, large 모델의 속도를 다시 측정했으며 v0.4 버전에서 다소 느려졌습니다.

- base 모델: 10.8 -> 14.4
- large 모델: 87.3 -> 165

## 사용자 사전

신경망 알고리즘은 소위 말하는 블랙박스 알고리즘으로 결과를 유추하는 과정을 사람이 따라가기가 쉽지 않습니다. 그래서 오분석이 발생할 경우 모델의 파라미터를 수정하여 바른 결과를 내도록 하는 것이 매우 어렵습니다. 이를 위해 khaiii에서는 신경망 알고리즘의 앞단에 기분석 사전을 뒷단에 오분석 패치라는 두 가지 사용자 사전 장치를 마련해 두었습니다.

### 기분석 사전

기분석 사전은 단일 어절에 대해 문맥에 상관없이 일괄적인 분석 결과를 갖는 경우에 사용합니다. 예를 들어 아래와 같은 엔트리가 있다면,

| 입력 어절 | 분석 결과    |
| --------- | ------------ |
| 이더리움* | 이더리움/NNP |

문장에서 `이더리움`으로 시작하는 모든 어절은 신경망 알고리즘을 사용하지 않고 `이더리움/NNP`로 동일하게 분석합니다.

세종 코퍼스에서 분석 모호성이 없는 어절들로부터 자동으로 기분석 사전을 추출할 경우 약 8만 개의 엔트리가 생성됩니다. 이를 적용할 경우 약간의 속도 향상도 있어서 base 모델에 적용하면 약 9.2초로 10% 정도 속도 향상이 있었습니다.

기분석 사전의 기술 방법 및 자세한 내용은 [기분석 사전 문서](https://github.com/kakao/khaiii/wiki/기분석-사전)를 참고하시기 바랍니다.

### 오분석 패치

오분석 패치는 여러 어절에 걸쳐서 충분한 문맥과 함께 오분석을 바로잡아야 할 경우에 사용합니다. 예를 들어 아래와 같은 엔트리가 있다면,

| 입력 텍스트 | 오분석 결과                             | 정분석 결과                                |
| ----------- | --------------------------------------- | ------------------------------------------ |
| 이 다른 것  | 이/JKS + _ + 다/VA + 른/MM + _ + 것/NNB | 이/JKS + _ + 다르/VA + ㄴ/ETM + _ + 것/NNB |

만약 khaiii가 위 "오분석 결과"와 같이 오분석을 발생한 경우에 한해 바른 분석 결과인 "정분석 결과"로 수정합니다. 여기서 "_"는 어절 간 경계, 즉 공백을 의미합니다.

오분석 패치의 기술 방법 및 자세한 내용은 [오분석 패치 문서](https://github.com/kakao/khaiii/wiki/오분석-패치)를 참고하시기 바랍니다.

**NNB NNP와 같은 단어의 뜻은 형태소 품사의 태그를 의미**

**CNN : 합성곱 신경망**

**khaiii는 데이터 기반으로 동작하기 때문에 기계학습 알고리즘(딥러닝)을 사용**

입력의 경우 각각의 음절이 분류 대상

문장을 띄어쓰기 단위로 토크나이징(토큰을 형태소단위로 쪼갬)한 뒤, 형태소와 품사를 붙인 형태로 결과 출력

**analyzed가 쪼개준 형태소 단위로 튜플을 만들고, 한 문장(혹은 구문)을 한 리스트로 합친다.**

**띄어쓰기에 취약(하지만 v.04에 따르면 띄어쓰기 오류에 대한 여러 모델을 실험하여 개선하였다고 합니다.)** 



## 오픈소스의 기능과, 이를 선정한 이유 ##

 카카오에서 개발한 한글의 형태소 분석 오픈소스인 khaiii는 문장을 띄어쓰기 단위로 토크나이징(토큰을 형태소단위로 쪼갬)한 뒤, 형태소와 품사를 붙인 형태로 결과 출력해주는 기능을 갖고 있으며, 이는 **analyzed가 쪼개준 형태소 단위로 튜플을 만들고, 한 문장(혹은 구문)을 한 리스트로 합쳐줍니다.** 또한, 신경망 알고리즘은 소위 말하는 블랙박스 알고리즘으로 결과를 유추하는 과정을 사람이 따라가기가 쉽지 않기 때문에 오분석이 발생할 경우 모델의 파라미터를 수정하여 바른 결과를 내도록 하는 것이 매우 어렵습니다. **이를 위해 khaiii에서는 신경망 알고리즘의 앞단에 기분석 사전을 뒷단에 오분석 패치라는 두 가지 사용자 사전 장치를 마련해 두었습니다. 입력의 경우 각각의 음절이 분류 대상입니다.**



**오픈소스 혹은 프로그램과 상호작용 초기 구상**

가공된 사용자의 검색 정보가 **django**로 온 시점부터 시작: 

가공한 정보가 도착하면 도착정보와 관련된 키워드를 해쉬태그와 매치,**sns**상에서 해쉬태그를 기반으로 추출한 자료들을**MaraiDB**에 전송, **MaraiDB**에서 보내 준 url을 통해 **scrapy**가 크롤링한 내용을 가져오고, 가져온 리스트안의 내용을 파이썬 한글 맞춤법 검사 라이브러리인 **Hanspell**을 이용하여 가공한 뒤에, 가공한 자료에서 **khaiii**를 이용하여 형태소 단위로 쪼개어 **MaraiDB**와 **scrapy**로 전송한 뒤, **MaraiDB**는 데이터베이스에 저장, **scrapy**는 분석한 키워드를 기반으로**place**를 크롤링한 후, 크롤링 한 내용을 **MaraiDB**로 전송 후, **django**에 자료 전송, 추가로 **elasticsearch**에도 쪼개진 키워드 제공가능, **elasticsearch**는 쪼개진 형태소로 색인어를 생성하고 색인어를 이용하여 빠르게 검색


## khaiii로 구상해본 기능들

---

사전추가기능도있음
형태소를 음절단위로 쪼개서 전달
기계학습기반 알고리즘
형태소를 분석하여  비슷한 형태소조합을 학습시켜  데이터를 쌓아가고 유사어, 동음이의어에 대해 뒤에오는 조사,어미,접미사 를 미리 유추 할 수 있게하여 (파싱)보다 정확한 검색을 도와주는 것을 기대 (동음어는 앞 뒤 내용에 따라 뜻이 천차만별로 바뀌기 때문에 학습이 필요) 

형태소 분석은 자연어 처리의 가장 기초적인 절차로 이후 구문 분석이나 의미 분석으로 나아가기 위해 가장 먼저 이루어져야 하는 과정이라고 볼 수 있다
Konlpy가 gpl 라이선스라 충돌을 막기위해 아파치2 라이선스인 khaiii사용 

기분석사전,오분석패치 이용하여 정확도개선 

기분석사전
기분석 사전은 단일 어절에 대해 문맥에 상관없이 일괄적인 분석 결과를 갖는 경우 사용합니다. 

사전 엔트리의 종류 

기분석 사전의 엔트리는 아래와 같은 두 가지 종류가 있습니다. 

• 완전 일치: 전체 어절이 완전히 일치하는 경우에 적용되는 엔트리 

• 전방 매칭: 어절의 앞부분부터 부분적으로 일치할 경우에도 적용되는 엔트리 

더 긴 문장을 우선으로 분리

기분석 사전                                  
기계학습 모델 실행 전에 적용 
기계학습 모델의 결과에 적용
분석 속도를 빠르게 함
오분석 패치   
분석 속도가 느려짐
단일 어절에 한해 적용 가능
어절과 형태소 개수에 제한이 없음


데이터 -> 모델 -> 오분석 패치/기분석 사전 

make_vocab(vocab.in/vocab.out 생성) -> vocabulary -> morph, resource->sentence(공백 masked 음절 단위로 쪼개진 문장) -> dataset(음절 to tensor) -> embedder  -> models ->  trainer -> evaluator -> tagger-> char_align -> trie 

오분석패치
오분석된 내용을 정분석하여 추출

아파치2.0 

코드를 수정한 경우, 표시
Apache 로고, 이름 등 상표 사용에 제한을 두고 있음
작업에 Apache 라이선스를 적용하려면 대괄호 "[]"로 묶인 필드를 자신의 식별 정보로 대체하여 다음 상용구 고지를 첨부



형태소 분석을 하는 이유는 주로 형태소 단위로 의미있는 단어 를 가져가고 싶거나 품사 태깅을 통해 형용사나 명사를 추출하고 싶을 때 많이 이용하게 됨 

Khaiii는 python을 사용을 할 수 있게 했지만 내부는 C언어로 돌아감 - 속도 때문에



## 시행착오와, 해결 과정 ## 

다른 오픈소스들과 연계에 관해 어긋나는 부분이 보여, 형태소 분석활용의 이유에 대해 조사를 해보니 보통 형태소분석 활용을 많이 하는 부분이 색인어 추출인데 일라스틱서치랑도 연계해서 스크래피가 긁어온 내용을 크롤링해서 긁어오고 검색이들어오면 카이가 쪼갠 단어로 색인어를 추출해서 일라스틱서치에 반환하고 반환받은 색인어를 이용해서 색인을 만든 뒤에 일라스틱서치가 자체 인풋 플러그인을 이용해서 빠르게 검색해서 스크래피로 전달하면 그 내용을 마리아 db를 거쳐 장고가 문서화하는 식으로 dfd설계를 살짝 바꾸는 것이 더 자연스러워 보였습니다. 그리고 스크래피에서 카이를 거치는 부분은 빼고 일라스틱 색인어를 만들어주는 방향으로 가는걸로 하였습니다. 또한, 그 글의 내용과 해시태그를 서로 비교해서 해당 글이 얼마나 신뢰성이 높은가를 체크해주는 것을 기대하여 사람들이 조회수를 올리려고 해쉬태그를 달면 실제 글이랑 해쉬태그랑 비교해서 글의 신뢰성을 판단하고 신뢰성이 낮으면 데이터베이스에 넣지않고 높다면 넣는 과정에서 비교하려면 실제 글을 형태소 분석해서 단어단위로 쪼개줘야하므로 카이가 필요하긴 하다고 생각했었습니다. 마지막으로 이를 장고에서 문서화 해주도록 하였습니다.

# py-hanspell

[![Build Status](https://camo.githubusercontent.com/e267848bb52dc3afbdb308a8505a483b89d490cbc48848436f82c5c20c6549e4/68747470733a2f2f7472617669732d63692e6f72672f737375742f70792d68616e7370656c6c2e7376673f6272616e63683d6d6173746572)](https://travis-ci.org/ssut/py-hanspell) [![PyPI version](https://camo.githubusercontent.com/5feb0662b811d03c5ea15ed29231d7d036e9125af5def168490dca1f1ec69821/68747470733a2f2f62616467652e667572792e696f2f70792f70792d68616e7370656c6c2e737667)](http://badge.fury.io/py/py-hanspell)

py-hanspell은 네이버 맞춤법 검사기를 이용한 파이썬용 한글 맞춤법 검사 라이브러리입니다.

파이썬 2.7 및 3.4 모두 호환됩니다.

------

## 설치

설치하는 방법으로는 두 가지 방법이 있습니다.

우선 pip를 이용해 설치하는 방법이 있습니다. 커맨드 라인에 다음 명령어를 입력하시면 자동으로 설치가 진행됩니다

```
$ pip install py-hanspell
```

다음으로 이 GitHub 저장소에서 직접 내려받아 설치하는 방법입니다. 이 저장소를 로컬에 clone 하거나 우측에 보이는 메뉴에서 zip 파일로 다운받은 후에 로컬 커맨드 라인에

```
$ python setup.py install
```

를 입력하시거나, 또는 hanspell 폴더를 자신의 프로젝트 폴더 안에 포함시키면 됩니다.

### 필요한 라이브러리

- requests

## 사용 방법

```
>>> from hanspell import spell_checker
>>> result = spell_checker.check(u'안녕 하세요. 저는 한국인 입니다. 이문장은 한글로 작성됬습니다.')
>>> result.as_dict()  # dict로 출력
{'checked': '안녕하세요. 저는 한국인입니다. 이 문장은 한글로 작성됐습니다.',
 'errors': 4,
 'original': '안녕 하세요. 저는 한국인 입니다. 이문장은 한글로 작성됬습니다.',
 'result': True,
 'time': 0.07065701484680176,
 'words': {'안녕하세요.': 2,
           '저는': 0,
           '한국인입니다.': 2,
           '이': 2,
           '문장은': 2,
           '한글로': 0,
           '작성됐습니다.': 1}}
>>> result
Checked(result=True, original='안녕 하세요. 저는 한국인 입니다. 이문장은 한글로 작성됬습니다.', checked='안녕하세요. 저는 한국인입니다. 이 문장은 한글로 작성됐습니다.', errors=4, words=OrderedDict([('안녕하세요.', 2), ('저는', 0), ('한국인입니다.', 2), ('이', 2), ('문장은', 2), ('한글로', 0), ('작성됐습니다.', 1)]), time=0.10472893714904785)
```

### `list`로 주고받기

```
>>> from hanspell import spell_checker
>>> spell_checker.check([u'안녕 하세요.', u'저는 한국인 입니다.'])
[Checked(result=True, original='안녕 하세요.', checked='안녕하세요.', errors=1, words=OrderedDict([('안녕하세요.', 2)]), time=0.03297615051269531),
 Checked(result=True, original='저는 한국인 입니다.', checked='저는 한국인입니다.', errors=1, words=OrderedDict([('저는', 0), ('한국인입니다.', 2)]), time=0.029018878936767578)]
```

### Checked

| attribute | -                                                          |
| --------- | ---------------------------------------------------------- |
| result    | 맞춤법 검사 성공여부를 나타냅니다.                         |
| original  | 검사 전의 문장입니다.                                      |
| checked   | 맞춤법 검사 후의 문장입니다.                               |
| errors    | 맞춤법 오류 수를 나타냅니다.                               |
| words     | [Checked.words](https://github.com/ssut/py-hanspell#words) |
| time      | 총 요청 시간을 나타냅니다.                                 |

### Checked.words

위 사용 방법에 나와있는 words 부분은 교정된 최종 문장을 공백으로 나눈(split) 결과입니다.

결과는 key가 단어, value가 [CheckResult](https://github.com/ssut/py-hanspell#results)를 나타냅니다.

아래 코드를 참고하세요.

```
>>> for key, value in result.words.items():
...    print(key, value)
안녕하세요. 2
저는 0
한국인입니다. 2
이 2
문장은 2
한글로 0
작성됐습니다. 1
```

### CheckResult

아래 코드로 import 하신 후 비교에 사용할 수 있는 상수입니다.

```
from hanspell.constants import CheckResult
```

| .CONST                  | -                                           |
| ----------------------- | ------------------------------------------- |
| .PASSED                 | 맞춤법 검사 결과 문제가 없는 단어 또는 구절 |
| .WRONG_SPELLING         | 맞춤법에 문제가 있는 단어 또는 구절         |
| .WRONG_SPACING          | 띄어쓰기에 문제가 있는 단어 또는 구절       |
| .AMBIGUOUS              | 표준어가 의심되는 단어 또는 구절            |
| .STATISTICAL_CORRECTION | 통계적 교정에 따른 단어 또는 구절           |

## 라이브러리 사용에 대한 안내

이 라이브러리는 네이버 한글 맞춤법 검사기를 바탕으로 만들어진 라이브러리입니다.

모든 결과 및 데이터에 대한 저작권 및 책임은 네이버 주식회사에 있으며, 라이브러리를 상업적으로 사용하거나 불법적인 용도로 활용한 부분에 대해서는 그 어떠한 부분에 대해서도 개발자는 책임지지 않습니다.

## 변경내역

- **버전 1.1**: list 타입으로 주고받는 기능 지원, 처리속도 향상
- **버전 1.0**: 첫 버전 릴리즈

## 라이선스(License)

py-hanspell은 MIT License로 제공됩니다. 

# 공공 데이터 및 해쉬태그를 통한 핫플레이스 추천 프로그램

# Django

작성자 : 1991297 - 박종범

+ Django란 무엇인가?
+ Django의 라이선스
+ Django의 장점과 한계점
+ Django MVT 패턴, 그리고 프로그램 설계 시 고려해야 할 내용
+ Django를 통해 구현해야 할 기능들 (View function)
+ Django가 다룰 데이터 구조에 대하여 (Model과 Template)



#### Django란 무엇인가?

> 'Ridiculously fast - Reassuringly secure - Exceedingly scalable'
>
> Django, The web framework for perfectionists with deadlines.



Python의 오픈소스 웹 프레임 워크로 **빠른 개발(rapid developement)**와 **명확함(clean)** 그리고 **실전적인 디자인(pragmatic design)**을 개발자에게 제공하고자 만들어진 오픈소스 소프트웨어입니다.



#### Django의 라이선스

해당 오픈소스는 **BSD 3-Clause "New" or "Revised"** 라이선스에 기반하고 있습니다. 이는 상업적 사용(Commercial use)가 가능하고, 수정(Modification), 개인적 사용(Private use)가 가능하나, 오픈소스를 포함한 다른 저작물과 마찬가지로 독점적 권리에 대한 주장(Warranty)가 불가능합니다.



#### Django의 장점과 한계점

✔ **GOOD!**

1. Python 기반 웹 프레임 워크이기 때문에 Python이 가진 강력한 장점들을 그대로 이용할 수 있습니다. (EX. 빠른 구현, 강력한 라이브러리, 쉽고 간결한 문법)
2. 진입장벽이 낮습니다. 별도의 데이터베이스 오픈소스 없이, 데이터베이스 테이블을 model.py 클래스로 작성하여 파이썬 코드만으로도 쉽게 제어할 수 있습니다. 또한 웹 서버를 설치하지 않고 파이썬 명령어 하나로 서버를 실행할 수 있습니다. (다만, 저희가 프로젝트에서 다루는 데이터는 용량이 크기 때문에, 별도의 데이터베이스 오픈소스를 구현하였습니다.)
3. 보안 기능을 기본적으로 제공해줍니다. 
4. 관리자 페이지를 기본적으로 제공해줍니다. 따라서 별도 구현 없이 페이지에 대한 관리를 손 쉽게 할 수 있습니다.
5. 많은 유틸리티들을 별도 구현없이 손쉽게 사용할 수 있습니다. 예를 들어 사용자 인증, 로그인 & 로그아웃 기능 등, 반드시 필요한 기능들에 대해서 별도 개발 없이 그대로 사용할 수 있습니다.



❌ **Bad...**

1. Python의 장점을 그대로 이용할 수 있는 것과 동시에, 파이썬이 지는 단점을 그대로 가지고 있습니다. 파이썬은 기본적으로 인터프리터언어이기 때문에 다른 언어로 구현한 프로그램에 비해서 상당히 느립니다.
2. 실시간 영상 및 채팅에 약합니다.  Django는 2003-2004년에 개발이 시작된 반면, 실시간 영상 및 채팅 기능은 그 이후 논의된 영역이기 때문에 해당 부분에 대해 초점을 맞추지 못한채로 오픈소스 소프트웨어가 구성되어 있습니다. 물론, 실시간 서비스를 제공하지 못하는 것은 아니지만, 다른 웹 프레임 워크에 비해서는 편의성이 떨어진다는 한계를 가지고 있습니다.

 

#### Django MVT 패턴, 그리고 프로그램 설계 시 고려해야 할 내용

`MVT`는 `Model`과 `View`, `Template`의 약어입니다. Django는 이 세가지 요소를 중점으로 프로그램을 운용하고 있습니다.

`Model`은 사용될 데이터에 대한 정의를 담고 있습니다. 

`Template`는 클라이언트에게 반환되는 HTML을 작성하는 부분입니다.

`View`는 요청을 받아, 해당 어플리케이션의 기능에 맞는 처리를 진행한 후, 그 결과 데이터를 템플릿처리하여 HTML로 된 응답 데이터를 웹 클라이언트로 반환하는 역할을 수행합니다.



이에 대한 시나리오는 다음과 같습니다

1. 유저가 서버로 해당 데이터를 요청합니다

2. 이를 URL dispatcher가 받아 View function으로 전송합니다.
3. View function은 미리 구성된 프로그램에 따라 이를 가공 및 처리합니다.
4. 그 과정에서 Model의 데이터를 생성, 수정, 삭제 등을 진행합니다.
5. 이에 대한 요청 처리 결과를 Template에 맞춰 생성합니다.
6. 종합적인 결과를 반환합니다.



**이를 근거로 제가 해당 프로젝트에서 고민했던 부분은**

1. **Model의 구성에 대해 고민해 볼 필요가 있었습니다.** 다른 오픈소스(혹은 내부 프로그램)과 상호작용 할 때, 어떠한 구조의 데이터로 상호작용 해야 할지에 대한 설계가 필요했습니다.
2. **View Function에 대해 고민해 볼 필요가 있었습니다.** 어떠한 기능을 구현해야 원하는 목표를 실현시킬 수 있을지에 대해 설계할 필요성이 있었습니다. (Ex) 크롤링한 데이터를 문서화하여 데이터베이스에 저장하는 기능)
3. **Template에 대한 고민이 필요했습니다.** 저희는 웹, 앱 크로스 플랫폼을 계획했기 때문에, Django를 통해 프론트 엔드를 구현하는 것이 아닌 React와 React Native를 통해 프론트 엔드를 구성하기로 결정하였습니다. 이 과정에서 Django와 React & React Native와의 상호작용이 어떻게 진행되어야 하는가에 대한 고민이 필요했습니다.



#### Django를 통해 구현해야 할 기능들 (View function)

1. ##### 자료의 신뢰성 평가를 진행하는 모듈

	`Scrapy` - `Khaii`와의 연계: 

 	`Scrapy`는 웹 크롤링 오픈소스 프로그램, `Khaii`는 게시글에 대해 형태소 분석을 하여 의미단위(명사, 동사, 형용사 등)로 쪼개주는 프로그램입니다.

	기존 여행 어플리케이션 혹은 SNS 서비스에서는, 사람들이 많은 조회수를 위해 무분별하게 해쉬태그를 작성하는 한계가 있었습니다. 저희는 신뢰성있는 정보를 사용자에게 추천하고자, 저희가 크롤링한 자료에 대해 신뢰도를 평가할 계획입니다. 해당 자료의 신뢰도가 저희가 설정한 기준점 이하로 떨어지게 되면, 해당 자료는 가공하지 않고 폐기하게 될 것입니다.

	해당 모듈은 `Scrapy`가 수집해 온 해쉬태그 데이터와, 그리고, `Khaii`가 게시글을 형태소 단위로 분석한 데이터를 서로 비교하여 신뢰도를 평가하게 됩니다. 즉, 해쉬태그에 대한 내용이 게시글에 전혀 언급이 되지 않았을 경우, 신뢰성 평가 모듈은 해당 게시글이 신뢰도가 낮다고 평가하게 되어 해당 폐기하게 됩니다. 



2. ##### 문서화를 진행하는 모듈 

	`Scrapy`가 크롤링한 해쉬태그와 URL, `Khaii`를 통해 가공한 관련 데이터, `공공 데이터 오픈 API`을 통해 받은 사고 관련 데이터들을 사전에 정한 데이터 구조(양식)에 맞춰 하나로 합치게 됩니다.  이와 같은 문서화가 진행된 정보(information)들은 `MariaDB`(데이터베이스관리프로그램)을 통하여 데이터 베이스로 전달하게 됩니다.

	문서화는 백 엔드에서 가공한 정보들을 프론트 엔드가 보다 손 쉽게 잘 가공하여 유저들에게 보여줄 수 있도록 양식에 맞춰 정보들을 분류, 정리해 놓는 과정입니다. 예컨대 웹 브라우저와 서버가 다양한 정보를 html 규격에 맞춰 클라이언트에게 보내는 것처럼, 저희 역시 가공한 정보들을 규격에 맞춰 프론트엔드에 보내고자 합니다.



3. ##### Scrapy를 제어하는 모듈

   **`Scrapy`에게 수집 기준점을 제시하는 기능**

	`Scrapy`를 제어하기 위해 `Scrapy`로 여러가지 정보를 제공하는 모듈이 필요할 것이라고 생각하게 되었습니다. 웹 환경은 상당히 넓기 때문에, 어떤것을 수집해야 하는지에 대한 명확한 근거가 필요할 것이라고 판단했습니다. 그렇기에 해당 모듈은 우선 `공공데이터 오픈 API`를 통해 여행 장소에 대한 자료를 제공받습니다. 주로 각 시도 기관에서 '전라남도_남도여행길잡이'와 같은 `CSV`, `JSON`형식의 파일을 오픈 API를 통해 제공하고 있습니다. 이를 Scrapy의 정보 수집 기준점으로 삼아, 보다 정확한 여행 자료를 수집할 수 있도록 돕고자 합니다.

	 뿐만 아니라 공공데이터 오픈 API를 통하여 사건사고 데이터를 받고 이를 `Scrapy`에게 수집 기준점으로 제시하여 사건 사고와 관련된 자료를 수집하고자 합니다.

		**`Scrapy`를 제어하는 기능**

  다음에는 Scrapy가 어느정도 간격으로 데이터를 수집할 것인지에 대한 설정이 필요할 것이라고 생각했습니다. 이에 대한 기준점을 쉽게 변경할 수 있도록 요청을 전달하는 기능을 구현하여 효율적으로 정보를 수집할 수 있도록 하고자 합니다.



4. ##### 프론트엔드와의 상호작용을 진행하는 오픈소스 DRF(Django REST framework)

	`DRF(Django REST Framework)`는 위와 같은 REST API서버를 쉽게 구축할 수 있도록 도와주는 오픈소스 소프트웨어입니다. 

	`Django`는 파이썬을 통해 구현된 웹 프레임워크이고, `React` 혹은 `React Native`는 자바스크립트로 구성된 프론트엔드 웹 프레임워크입니다. 따라서 `React`와 연동하기 위해 `Django`를 통해 가공한 데이터들을 `json`형식으로 바꾼 후, 미리 설정한 규약 혹은 포맷에 따라 데이터를 전송할 수 있도록 설정할 필요가 있습니다. 그렇기에 해당 오픈소스 소프트웨어에 근거하여 상호간 통신이 원활히 이루어 질 수 있도록 모듈을 구성해야 할 필요성이 있다고 판단하였습니다.



#### Django가 다룰 데이터 구조 설계에 대하여 (Model과 Template)

	우선 Scrapy가 무엇을 수집(crawling)하는 지에 대해 설명할 필요가 있습니다. Scrapy는 장소에 대한 키워드를 근거로 키워드와 일치하는 해쉬태그를 찾고, 이를 통해 해쉬태그와 해쉬태그가 달린 게시글, 그리고 해당 URL을 수집하게 됩니다. 즉 `해쉬태그`, `게시글`, `URL` 이 세가지 자료를 수집하여 프로그램에게 제공하게 됩니다.

  이때 Scrapy가 수집한 자료 중 `게시글`은 Khaii가 형태소 단위로 분석하여 재 가공하게 됩니다. 주로 주어(subject) 위주로 형태소를 리스트 형식으로 재 구성하여 문서화를 진행하는 모듈에게 전달하게 됩니다.

	문서화 모듈은 해쉬태그, 게시글 형태소 리스트, URL을 받아 하나의 문서화를 진행함과 동시에 추가로 해당 장소에서 발생할 수 있는 사고 관련 키워드, 관련 사고에 대한 내용들이 서술된 URL등을 추가하여 문서화를 완료합니다.

	즉, Django가 다룰 데이터 내용 및 구조는 `해쉬태그`, `관련 정보가 담긴 형태소 리스트`, `해당 URL`, `사건사고 키워드`, `해결책에 대한 URL`, `신뢰도` 이며, 이는 데이터베이스에 저장되었다가 요청 시 제공하는 것으로 프로그램 구성을 계획했습니다.





#### 참조

작성자는 아래 URL을 참고, 인용하여 해당 문서를 작성하였습니다.

Django 공식 홈페이지

https://www.djangoproject.com/  

위키백과 'Django 검색'

https://ko.wikipedia.org/wiki/%EC%9E%A5%EA%B3%A0_(%EC%9B%B9_%ED%94%84%EB%A0%88%EC%9E%84%EC%9B%8C%ED%81%AC) ****

멍토 : 장고를 왜 사용할까(특징, 장단점)

https://mungto.tistory.com/302

책 읽는 개발자_테드 : 장고를 사용하는 이유 및 작동 구조를 알아보자

https://scshim.tistory.com/560

헬스하는 감성 개발자 : [Django] 장고의 장점(vs Flask, Pyramid)

https://healthdevelop.tistory.com/entry/Django-%EC%9E%A5%EA%B3%A0%EC%9D%98-%EC%9E%A5%EC%A0%90vs-Flask-Pyramid-Python-%EC%9B%B9-%ED%94%84%EB%A0%88%EC%9E%84%EC%9B%8C%ED%81%AC

인코덤(incodom.kr) : DRF

http://www.incodom.kr/Django/DRF





# MariaDB

1991297 - 박종범

+ MariaDB란?
+ MariaDB의 라이선스
+ MariaDB를 선택한 이유
+ MariaDB에 무엇이 저장되는가
+ MariaDB에 저장된 자료는 이후 어떻게 활용되는가?



#### MariaDB란? 

관계형 데이터 관리 시스템(RDBMS)입니다. MySQL의 불확실한 라이선스 상태에 반발하여 오픈소스 진영 개발자들이 2009년 발표하여 개발이 진행중인 오픈소스 소프트웨어입니다.



#### MariaDB의 라이선스

**MariaDB는 GPL V2 라이선스를 채택하고 있습니다.**  해당 소프트웨어의 소스코드를 수정 및 포함하여 배포 시, 해당 프로그램의 소스코드를 모두 공개해야 할 의무가 있습니다. 

다만, 다른 데이터베이스관리프로그램이 그렇듯이 서버에서 돌아갈 뿐, 해당 소스코드가 프로그램에 포함되어 수정 및 배포되는 것이 아니기때문에, 해당 프로그램의 소스코드를 공개해야 할 의무는 없습니다. 다만 유료버전을 구매하여 문제에 대한 솔루션을 제공받을 수는 있습니다.



#### MariaDB를 선택한 이유

위에 언급한 것처럼,  MySQL는 오픈소스에 대해 호의적이지 않은 오라클로 넘어갔습니다. 따라서 MySQL는 라이선스가 변경될 수 있다는 불안요소를 가지고 있습니다. 그렇기에 비교적 이러한 부분에서 자유로운 MariaDB를 선택하게 되었습니다.

  또한, MySQL에 비해 MariaDB는 보다 가볍다는 장점을 가지고 있습니다. 또한, 기존 주류였던 MySQL과 완벽한 호환이 가능하다는 장점 역시 존재합니다.

  

#### MariaDB에 무엇이 저장되는가?

MariaDB에는 4가지 요소가 저장됩니다.

첫번째는 해쉬태그입니다. 웹 크롤러 Scrapy가 인스타그램, 네이버 플레이스 등 여행 관련 사이트에서 긁어 온 해쉬태그를 별도로 저장합니다. 

두번째는 URL입니다. 해당 해쉬태그가 작성되어있던 링크를 저장합니다.

세번째는 리뷰를 형태소 분석한 것입니다. 웹 크롤러 Scrapy가 수집해 온 리뷰를 hanspell이 맞춤법 검사 후, khaiii를 통해 리뷰를 품사 별로 분류하게 됩니다.

네번째는 사고 정보입니다. 공공 데이터에서 가져온 사고 정보를 해쉬태그와 비교하여, 해당 장소에서 발생한 사고 정보를 찾아내고, 이를 데이터베이스에 저장하게 됩니다.

이렇게 4개의 요소가 하나의 정보를 이루게 됩니다. 



#### MariaDB에 저장된 자료는 이후 어떻게 활용되는가?

우선 신뢰도 평가를 위해 사용됩니다. 저희는 리뷰 내에 해쉬태그에 해당되는 키워드가 많이 포함될 수록 해당 리뷰의 신뢰도가 높다고 판단하기로 했습니다. 그렇기에 형태소 분석된 리뷰와 해쉬태그를 꺼내어 비교함으로서, 해당 정보의 신뢰도가 높은지 낮은지 판단할 수 있겠습니다.

다음 사고 정보 추출을 위해 사용됩니다. 공공데이터API는 각 장소에서 발생한 사고의 종류와 횟수를 다양한 형식으로 제공하고 있습니다. 이러한 빅 데이터를 미리 크롤링한 해쉬태그와 비교하여, 공공데이터API에서 가져온 빅데이터로부터 유저들에게 제공해야 할 사고 정보를 추출해 냅니다.

위와 같은 작업이 끝나면 4가지 요소 템플릿에 맞춰 구성한 뒤, 프론트에 보내게 됩니다.



#### 출처

https://ko.wikipedia.org/wiki/MariaDB

데이터베이스 비교 분석

https://benlee73.tistory.com/178

# React-Next.js, React Native, Naver Maps API

작성자: 1891144 - 김민규

+ React 소개
+ React의 장단점 및 주요 기능
+ Next.js 소개
+ Next.js의 특징 및 주요 기능
+ React Native 소개
+ React Native의 장단점 및 주요 기능
+ React-Next.js, React Native로 나타나는 서비스 화면 형태
+ 다른 오픈소스와의 연계
+ Naver Maps API 소개
+ Naver Maps API의 장단점 및 주요 기능
+ Naver Maps API를 선택한 이유
+ 라이선스 정보
+ 참조

#### React 소개

React혹은 React.js는 **컴포넌트를 기반으로 한** 유저 인터페이스를 생성하는 **프리 오픈소스 자바스크립트 라이브러리**로 메타(페이스북)에서 관리하고 있습니다.

#### React의 장단점 및 주요 기능

✔ **GOOD!**

1. UI를 컴포넌트라는 작은 단위로 구분하여 사용자의 목적에 맞는 높은 재사용성을 확보합니다.
2. 선언적 프로그래밍을 통하여 프로그램의 절차에 집중하는 명령적 프로그래밍과는 달리 달성해야 하는 결과에 집중합니다. 이는 코드를 예측 가능하게 하고 디버깅을 보다 쉽게 합니다.
3. 브라우저가 html을 전달받아 파싱하는 과정에서 DOM이라는 렌더 트리가 형성됩니다. 모든 렌더링 과정에 이러한 실제 DOM이 조작되면 성능의 저하가 일어날 수 있습니다. React는 가상 DOM을 사용하여 변동 사항만 실제 DOM에 적용함으로서 실제 DOM이 작동되는 경우를 줄여 렌더링 성능이 저하되는 것을 막아줍니다.
4. 양뱡향 데이터 바인딩은 UI와 데이터 상호간의 변화가 가능하여 사용자가 동기화에 대하여 신경 쓰지 않고 프로그래밍 할 수 있지만 성능 저하가 발생합니다. 반면 React는 단방향 데이터 바인딩을 사용하여 데이터에서 UI를 갱신하도록 하고 UI에서 데이터를 갱신할 때는 이벤트 처리를 통해 하도록 설계되었습니다. 이로 인하여 성능의 이점을 얻습니다.

❌ **Bad...**

1. React는 Vue.js, Angular 등과 같은 프레임워크가 아닌 라이브러리 입니다. 따라서 React 자체만으로 전역 상태 관리, 라우팅, 빌드 시스템 등을 지원하지 않습니다. 때문에 이러한 기능들을 사용하기 위하여 추가적인 라이브러리를 붙여야 합니다. 이는 사용자로 하여금 필요한 부분만 가져다 쓸 수 있다는 점에서 장점이 되기도 하지만 React에 대한 진입 장벽을 높이는 원인이기도 합니다.
2. React는 jsx(JavaScript XML) 형태로 코드를 작성하는데 자바스크립트만을 사용해 UI 로직과 DOM을 구현합니다. 이는 작성에 있어 높은 자유도를 보장하지만 코드 가시성의 하락으로 이어질 수 있습니다.

#### Next.js 소개

앞서 설명하였듯이 React는 프론트엔드 프레임워크가 아닌 라이브러리로서 UI라는 작은 영역을 담당합니다. 이로 인하여 비교되는 다른 프론트엔드 프레임워크가 지원하는 기능들을 갖추기 위해서 추가적인 라이브러리와 결합하는 것을 필요로 합니다. (ex: React-Redux, React-Router) Next.js는 React로 만들어진 **프론트엔드 웹 프레임워크**로서 **추가적인 강력한 기능들**을 제공합니다.

#### Next.js의 특징 및 주요 기능

1. 기존 React는 CSR(클라이언트 사이드 렌더링) 방식만을 지원합니다. CSR은 클라이언트(브라우저)에서 웹 서버로부터 html, js, css등 각종 자원을 받아 처음 1회의 렌더링을 하고 이후 변경된 부분에 있어서 서버에 요청하여 JSON파일을 받아오는 방식입니다. 이는 트래픽을 감소시키고 서버의 부담을 줄이는 장점을 가지지만 초기 로딩 속도가 느려지는 단점을 가지고 있습니다. 반면 Next.js는 SSR(서버 사이드 렌더링)을 지원합니다. SSR은 서버에서 모든 렌더링을 마치고 클라이언트로 데이터를 보내주도록 합니다. 이를 통하여 초기 로딩 속도를 개선할 수 있고 이는 각 사용자별로 혹은 시기 별로 다른 정보를 추천해줘야 하는 본 서비스에 적합합니다. 추가적으로 CSR은 클라이언트가 자바스크립트를 실행시켜야 페이지의 전체 view가 나타나기에 자바스크립트를 읽지 못하는 웹 크롤러가 html만을 읽어 SEO(검색 엔진 최적화)가 힘들어집니다. 반면 SSR은 서버에서 미리 렌더링 하여 데이터를 보내므로 전체 view를 검색 엔진(네이버, 구글)의 웹 크롤러가 읽기 수월해지므로 검색 엔진이 쉽게 발견하도록 하여 들어오는 트래픽의 양과 질이 높아지는 이득을 꾀할 수 있습니다. 이는 Next.js가 가지는 대표적인 장점입니다.
2. 이외에 폴더, 파일의 자동 라우팅 기능(Automatic Routing), 리소스를 분석하여 꼭 필요로 하는 js파일을 우선적으로 보내는 기능(Automatic Code Splitting)이 있습니다.

#### React Native 소개

모바일 앱의 성능과 안정성을 향상시키기 위해선 ios와 안드로이드 각각의 플랫폼에서 제공하는 개발 환경에 따라 설계하는 것이 최선이지만 초기 사업 과정에서 서비스를 배포 및 유지 보수에 있어 상당한 비용이 소모될 수 있습니다. 이를 개선하기 위하여 ios와 안드로이드 모두에 사용할 수 있는 **크로스플랫폼 프레임워크**가 등장하였고 React Native는 이러한 프레임워크 중 하나입니다.

#### React Native의 장단점 및 주요 기능

✔ **GOOD!**

1. 코드 푸쉬 사용이 용이합니다. React Native는 프로그램의 이면에서 돌아가고 있는 비즈니스 로직, 사용자에게 비춰지는 부분인 view 로직을 모두 자바스크립트 하나로 작성합니다. 그리고 이를 앱스토어나 플레이스토어에 배포하기 위한 바이너리 안에 자바스크립트 번들을 포함시키는 구조를 가지고 있습니다. 앱스토어나 플레이스토어는 정책상 앱에 포함되어 있는 자바스크립트 구조를 동적으로 변경하는 것을 허용하고 있습니다. 이는 앱 업데이트 없이 앱의 내용을 변경할 수 있도록 하여 유저들이 굳이 업데이트를 하지 않아도 서비스를 일정하게 유지하도록 해줍니다.
2. React와 일부 html, css관련 문법을 제외하고 완전히 동일하게 사용할 수 있습니다. 이는 React 스택에서 추가 적용하는데 드는 러닝 커브를 상당부분 줄여줍니다.

❌ **Bad...**

1. 공식 라이브러리 생태계가 빈약합니다. 예를 들어 React Native는 화면의 전환을 담당하는 Navigation 라이브러리를 공식적으로 지원하고 있지 않습니다. 단 하나의 화면만 가지고 있는 앱을 제작하지 않는 점에서 이러한 부분을 공식적으로 제공하지 않는다는 점은 아쉬운 부분입니다.

#### React-Next.js, React Native로 나타나는 서비스 화면 형태

본 서비스는 웹은 React-Next.js, 앱은 네이티브 부분은 React Native로 설계한 후 웹뷰를 통해 작동되도록 하는 하이브리드 앱으로 구상하였습니다. 앱, 웹을 통틀어 유저가 처음 접속하여 보게 되는 메인 화면은 검색창과 밑에 접속 시에 가장 많이 태그되고 있는 키워드 순으로 워드 클라우드가 형성된 모습을 구상하였습니다. 이를 통하여 유저로 하여금 현재 가장 핫한 키워드 순으로 장소를 보여줄 수 있도록 하였습니다. 여기서 검색창을 클릭하였을 시 화면이 전환되어 Naver Maps API를 통한 지도와 각 장소에 카테고리에 맞는 검색 옵션을 조정할 수 있는 부분이 출력됩니다. 장소를 검색 시에 그 장소와 그 주위 다른 장소에 대한 마커가 표현되어 있는데 특정 마커를 클릭하거나 터치할 시 `장소명`, `주소`, `해당 장소의 URL`, `사건 사고 키워드`, `해결책에 대한 URL`, `리뷰 정보`, `즐겨 찾기`가 출력되어 특정 장소에 대한 정보를 알 수 있도록 하고 로그인한 유저는 즐겨 찾기를 통해 관심있는 장소를 저장하여 쉽게 그 장소에 대한 정보를 파악하도록 구성하였습니다.

#### 다른 오픈소스와의 연계

#### 1. Django와의 연계

React와 Django간의 Rest API 서버를 통하여 통신합니다. 데이터의 전달은 크게 이러한 부분으로 구성됩니다.

* 유저 정보의 전달

로그인 시 Django에 유저 정보를 호출하고 유저 정보 데이터를 받습니다. 회원가입 시 생성된 유저 정보를 Django로 전달합니다.

* 장소 정보의 전달

위에 서술된 장소명, 주소, URL, 리뷰 정보 등을 호출하여 데이터를 받아와 각 장소에 맞는 페이지를 렌더링하여 사용자에게 보여줍니다.

#### 2. Naver Map API와의 연계

Django로부터 받아온 장소 정보는 Naver Map API로도 연결됩니다. 받아온 주소 정보를 Geocoder 서브 모듈의 Service 객체를 사용하여 좌표값을 추출합니다. 이를 입력하여 마커를 생성하고 마커를 클릭하거나 터치할 경우 해당 마커의 정보창이 표시되어 `장소명`, `주소`, `해당 장소의 URL`, `사건 사고 키워드`, `해결책에 대한 URL`, `리뷰 정보`, `즐겨 찾기`가 출력됩니다.

#### 3. Elastic Search, Kospeech와의 연계

API 키값을 연계하면 Elastic Search 자체에서 지원하는 UI를 사용할 수 있습니다. 이를 통해 Elastic Search에 저장되어 있는 데이터 테이블을 출력하여 이용할 수 있습니다. 또한 react-speech-kit을 통하여 얻은 음성인식 데이터를 kospeech에 전달하여 음성인식을 통한 검색을 도모할 수 있습니다.

#### 4. reCATCHA와의 연계

회원가입 시 봇을 판별하기 위해 쓰입니다. reCAPTCHA에서 사용자가 전달한 토큰을 미리 발급받은 비밀 키값과 함께 구글 api에 검증 요청을 하고 검증 결과를 JSON으로 반환 받아 처리합니다.

#### Naver Maps API 소개

네이버 지도를 활용하여 지도 관련 웹 서비스나 애플리케이션에 간단한 약도부터 유명 관광지 표시 등 다양한 정보를 표시할 수 있는 API 서비스 입니다.

#### Naver Maps API의 장단점 및 주요 기능

✔ **GOOD!**

1. 데스크탑과 모바일 환경 모두 부드러운 화면 인터렉션을 제공합니다.
2. 360도 파노라마 뷰를 제공합니다.
3. 교통상황, 마커&인포윈도우, 데이터레이어 등 다양한 표현을 지원합니다.
4. 다양한 웹 브라우저에 대한 호환성이 있습니다.

❌ **Bad...**
1. 이용 한도에 따라 요금을 청구하는 유료 서비스 입니다.

#### Naver Maps API를 선택한 이유

본 서비스에서 지도에 요구하는 기능은 크게 **장소명, 주소에 대한 검색기능**, 특정 장소에 대한 **마커 표시 기능**, 특정 장소의 마커를 클릭 혹은 터치 하였을 때 **이벤트 창 출력 기능**입니다. 처음에는 OpenLayers라는 OpenStreetMap 기반의 오픈소스 지도 라이브러리를 염두하였습니다. 그러나 OpenLayers는 한국 지리에 대한 정보가 상대적으로 빈약하고 자체적인 검색 기능을 가지고 있지 않다는 단점을 가지고 있었습니다. 이러한 부분은 장소를 나타내고 추천해야 하는 본 서비스에 다소 치명적인 부분이라고 판단하여 한국 지리에 대한 다양한 정보를 제공하고 가장 많은 사람들이 이용하는 지도 서비스이기에 상대적으로 친숙한 Naver Maps API를 선택하였습니다.

#### 라이선스 정보

React, Next.js, React Native 모두 MIT 라이선스에 기반하고 있습니다. 본 라이선스는 소프트웨어의 사용, 복제, 변경, 통합, 발행, 배포, 재실시, 판매에 대한 제약을 포함한 어떠한 제약 없이 본 소프트웨어를 취급할 수 있으며, 이는 제3자에 대한 소프트웨어 지급시에도 동일하게 적용됩니다. 단 라이선스 표시를 소프트웨어 및 모든 복제품에 표시해야 하며 소프트웨어와 관련한 어떤 법적 책임을 지지 않습니다.

#### 참조

React 공식 한국어 홈페이지 https://ko.reactjs.org/
Vue vs React in 2022 - Comparison of Two Most Popular JS Frameworks https://www.monterail.com/blog/vue-vs-react
React vs Vue: The Core Similarities and Differences https://mentormate.com/blog/react-vs-vue-the-core-differences/
코린이 형준: 프로젝트(1)프론트, 백엔드 통신 방법(feat.django, react) https://leeceo97.tistory.com/54

Next.js 공식 홈페이지 https://nextjs.org/
빠리의 택시 운전사: React 개발이 이렇게 쉬웠나? (Feat. Next.js) https://geonlee.tistory.com/229
ivorycode: Next.js를 사용하는 이유 https://ivorycode.tistory.com/entry/Nextjs%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EC%9D%B4%EC%9C%A0

React Native 공식 홈페이지 https://reactnative.dev/
토스ㅣSLASH 22 - 미친 생산성을 위한 React Native https://www.youtube.com/watch?v=b_6CjuvVg8o

Naver Cloud의 Naver Maps API 상품 페이지 https://www.ncloud.com/product/applicationService/maps
Naver Maps API v3 공식 홈페이지 https://navermaps.github.io/maps.js/

MIT 라이선스 위키백과 https://ko.wikipedia.org/wiki/MIT_%ED%97%88%EA%B0%80%EC%84%9C

