# HotplaceRecommand

# Scrapy

작성자 : 1971352 - 김기범



+ Scrapy를 고르게된 이유
+ Scrapy의 라이선스
+ Scrapy 프로그램 설계 시 고려해야 할 내용
+ 타 프로그램과의 상호작용



#### Scrapy를 고르게된 이유

해시태그기반 핫플추천이라는 주제에 대해 생각해보니 우선, 그 사이트를 분석해주는 오픈소스가 필요하다는 것을 생각하게되었고 그 과정에서 찾은 크롤링 오픈소스는 2가지였습니다. 

​	하나는 Selenium 또 다른 하나는 Scrapy입니다. 셀레니움은 페이지를 렌더링 하기 위해 필요한 js, css 그리고 image 파일까지 불러오며 속도가 많이 지연된다. 하지만 scrapy는 지정된 url만 조회합니다. 그렇기 때문에 scrapy가 셀레니움보다 가볍고 빠른 퍼포먼스를 낼 수 있는것이 첫번째 이유입니다. 

​	인스타그램을 크롤링 할때에는 주식자료처럼 동적이 아닌 정적인 자료이다 보니 scrapy가 좀더 효율적이라고 판단 하였습니다. 

​	우려했던 부분인 Scrapy는 lxml을 기반으로 만들어져 있어서 자바스크립트를 읽고 실행하지 못한다 라는 단점은 인스타그램이 파이썬기반인 것을 알게 되어서 괜찮다고 생각하였습니다. (자바스크립트를 처리하기 위해 Selenium과PhantomJS를 주로 사용하는데, Scrapy에서도 Splash를 사용하면 자바스크립트를 처리할 수 있다.) 마지막으로 Django 와 같은 백엔드 서비스와 연동하기도 좋고, 플러그인도 다양해서 여러모로 만능인 프레임워크이기에 선택하게되었습니다.



#### Scrapy란 

**스크래피**(Scrapy)는 Python으로 작성된 오픈소스 웹 크롤링 프레임워크입니다. 웹 데이터를 수집하는 것을 목표로 설계되었습니다. 또한 API를 이용하여 데이터를 추출할 수 있고, 범용 웹 크롤러로 사용될 수 있습니다.

Scrapy 프로젝트는 "spiders"를 중심으로 개발되었습니다. "spiders"는 여러 기능이 내장된 크롤러이다. 장고와 같은 철학인 중복배제를 따르고 있는 프레임워크입니다. Scrapy는 개발자들이 코드 재사용성을 높일 수 있도록 도와주어, 큰 규모의 크롤링 프로젝트 개발을 쉽게 할 수 있도록 해줍니다. 또한 Scrapy는 개발자들이 크롤링하려는 사이트의 동작을 테스트할 수 있도록 웹 크롤링 셸을 제공합니다.



#### Scrapy의 라이선스

해당 오픈소스는 **BSD 3-Clause "New" or "Revised"** 라이선스에 기반하고 있습니다. BSD 2-Clause 라이선스인  유사하지만 다른 사람이 서면 동의 없이 파생 제품을 홍보하기 위해 저작권 보유자 또는 기여자의 이름을 사용하는 것을 금지하는 3번째 조항이 포함된 허용 라이선스입니다.

BSD 2-Clause

- 저작권자 표기. BSD 라이선스가 걸린 소프트웨어를 이용하는 경우, 해당 저작권자의 이름과 BSD 라이선스의 내용을 같이 배포해야 한다.

- 보증 부인. BSD 라이선스가 걸린 소프트웨어로 인하여 법률상/도의상 피해가 발생하더라도, 그 책임을 저작권자에게 물을 수 없다.

  

#### Scrapy 프로그램 설계 시 고려해야 할 내용

`Scrapy`는 프레임 워크이기때문에 주어진 규칙에 따라 프로젝트를 진행해야 합니다. 그 과정속에서 크게 `Spider`와 `Item`, `Item pipeline`이 3가지로 진행됩니다. 

`Spider`는 어떤 웹 사이트들을 어떠한 규칙에 의거하여 크롤링할 것인지 명시하고, 각각의 웹 페이지의 어떤부분을 스크래핑할 것인지 등을 일괄적으로 명시하는 클래스입니다. 즉. 웹크롤링과 스크래핑 규칙을 설정 하는 핵심요소입니다. 여기서 좀더 자세히 나가자면 웹페이지를 가지고 올때, 일반적으로 수행해야 할 작업은 HTML소스에서 데이터를 추출하는 것입니다. 이를 위해 Scrapy에는 데이터 추출을 위한 자체 매커니즘이 있습니다. XPath또는 CSS표현식으로 지정 된 HTML 문서의 특정 부분을 선택하기 때문에 Selector라고 합니다.

`Item`은 Scrapy에서 기본 제공하는 자료구조 클래스입니다. 새로운 Item 클래스를 정의하고 여기에 우리가 수집하고자 하는 정보들을 명시하면, Spider 상에서 실제 스크랭핑을 수행환 결과물을, 파일형태로 저장할 때, item들을 어떻게 처리할 것인지 명시하면, 해당 규칙에 의거하여 데이터를 가공하거나 혹은 외부 파일로 간편하게 저장할 수 있습니다.

`Item pipeline`는 클래스를 새로 정의하고 여기에 각 Item들을 어떻게 처리할 것인지 명시하면, 해당 규칙에 의거하여 데이터를 가공하거나 혹은 외부 파일로 간편하게 저장할 수 있습니다.

진행과정은 다음과 같습니다

1. 새로운 Scrapy 프로젝트 생성

2. Item 정의

3. Spider 작성

   3-1 Selectors 설정 

4. Item pipeline



**이 과정 속에서 구체적으로 생각해야 되는것은**

1. **Item및 Item pipeline구성에 대해 고민해 볼 필요가 있었습니다.** 크롤링시, 불필요한 데이터도 있기때문에 무슨 데이터가 필요하며 그것을 어떤식으로 처리할지에 대해 고민하였습니다.
2. **크롤링 범위에 대해 고민할 필요가 있었습니다.**  모든 사이트를 크롤링 하겠다라는 것은 너무 광범위하고 허황된 이야기 인것같아서 시스템 구상 단계에서 생각한 범위는 우선 공공데이터 오픈 API 를 통해 여행 장소에 대한 자료를 제공받습니다. 주로 각 시도 기관에서 여행관련된 CSV , JSON 형식의 파일을 오픈 API를 통해 제공하고 있습니다. 이를 Scrapy의 정보 수집 기준점으로 삼아, 보다 정확한 여행 자료를 수집할 수 있도록 돕고자 합니다.
   뿐만 아니라 공공데이터 오픈 API를 통하여 사건사고 데이터를 받고 이를 Scrapy 에게 수집 기준점 으로 제시하여 사건 사고와 관련된 자료를 크롤링하고자 합니다.
3. **크롤링시 허용되는 범위에 대해 고민해 볼 필요가 있었습니다.**  무분별한 크롤링시 법적 문제까지 갈수도있다는 글을 본뒤로 크롤링시 허용되는 범위에 대해 조사할 필요성을 느꼈습니다. 그에 따라 조사하던중 robots.txt를 사용한다면 이러한 문제를 예방할 수 있다고 합니다.



#### 타 프로그램과의 상호작용

웹사이트 크롤링(scrapy)>정규표현식으로 파싱한 후, 내용 분석(khaiii)->분석된 내용과 url주소등을 Django에게 보냄->데이터베이스에 저장(Django)

이것이 현재 제 파트에서 이루어지는 과정입니다. 따라서 첫번째 과정에서 검색한 키워드를 받게되면 Scrpay를 통해 웹크롤링을 진행하여 데이터를 얻습니다. ex) 공공데이터 오픈 API

 그후 이 데이터를 정규 표현식으로파싱합니다. 이 파싱한 내용을 형태소 분석기를 통해 내용을 분석하는 과정을 거칩니다. 그후 분석된 자료(해시태그, 게시글)와 url주소를 데이터베이스에 저장합니다. 또한 데이터를 호출하거나 저장할때 Django와의 연계할수있는 기능들이 있다고하여 보다 쉽게 데이터를 처리할수있다고 예상됩니다.



#### 참조

작성자는 아래 URL을 참고, 인용하여 해당 문서를 작성하였습니다.

Scrapy 깃허브 주소

https://github.com/scrapy/scrapy



Scrapy 위키백과

https://ko.wikipedia.org/wiki/%EC%8A%A4%ED%81%AC%EB%9E%98%ED%94%BC_(%EC%9B%B9_%ED%94%84%EB%A0%88%EC%9E%84%EC%9B%8C%ED%81%AC)



Scrapy와 selenium차이점

https://keyhyuk-kim.medium.com/python-%EC%9B%B9-%ED%81%AC%EB%A1%A4%EB%9F%AC-%EB%8F%84%EA%B5%AC-%EB%B9%84%EA%B5%90-%EB%B0%8F-%EC%82%AC%EC%9A%A9-%ED%9B%84%EA%B8%B0-scrapy-vs-selenium-vs-requests-urllib-6483041ca1ba

Scrapy란(프레임워크와 라이브러리에 대한 정보)

https://curriculum.cosadama.com/scrapy/1-1/



scrapy 소개

https://jybaek.tistory.com/927



Scrapy 기본구조와 간단한 웹 스크래핑

https://nittaku.tistory.com/135



정의 및 Django와의 연계활용예시

(https://blog.theodo.com/2019/01/data-scraping-scrapy-django-integration/)

셀렉터 정의및 예시

https://overit.tistory.com/entry/Scrapy-Selector

크롤링 활용예시

[https://pycoding.tistory.com/entry/scrapy%EC%8A%A4%ED%81%AC%EB%9E%98%ED%94%BC-%ED%81%AC%EB%A1%A4%EB%A7%81-%EC%8B%A4%EC%A0%84-csv-%EC%A0%80%EC%9E%A5%EA%B9%8C%EC%A7%80](https://pycoding.tistory.com/entry/scrapy스크래피-크롤링-실전-csv-저장까지)

 https://hansuho113.tistory.com/7 

**[Django] Celery,Redis로 Scrapy 크롤링 주기적으로 하기 (feat. Docker)**

 ( https://qwlake.github.io/django/2020/05/15/django-scrapy-with-celery-redis/ )
