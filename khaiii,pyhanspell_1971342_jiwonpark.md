# khaiii

khaiii는 "Kakao Hangul Analyzer III"의 첫 글자들만 모아 만든 이름으로 카카오에서 개발한 세 번째 형태소분석기입니다. 두 번째 버전의 형태소분석기 이름인 dha2 (Daumkakao Hangul Analyzer 2)를 계승한 이름이기도 합니다.

형태소는 언어학에서 일정한 의미가 있는 가장 작은 말의 단위로 발화체 내에서 따로 떼어낼 수 있는 것을 말합니다. 즉, 더 분석하면 뜻이 없어지는 말의 단위입니다. 형태소분석기는 단어를 보고 형태소 단위로 분리해내는 소프트웨어를 말합니다. 이러한 형태소분석은 자연어 처리의 가장 기초적인 절차로 이후 구문 분석이나 의미 분석으로 나아가기 위해 가장 먼저 이루어져야 하는 과정으로 볼 수 있습니다.

## 데이터 기반

기존 버전이 사전과 규칙에 기반해 분석을 하는 데 반해 khaiii는 데이터(혹은 기계학습) 기반의 알고리즘을 이용하여 분석을 합니다. 학습에 사용한 코퍼스는 국립국어원에서 배포한 [21세기 세종계획 최종 성과물](https://ithub.korean.go.kr/user/noticeView.do?boardSeq=1&articleSeq=16)을 카카오에서 오류를 수정하고 내용을 일부 추가하기도 한 것입니다. 전처리 과정에서 오류가 발생하는 문장을 제외하고 약 85만 문장, 천만 어절의 코퍼스를 사용하여 학습을 했습니다.

## 알고리즘

기계학습에 사용한 알고리즘은 신경망 알고리즘들 중에서 Convolutional Neural Network(CNN)을 사용하였습니다. 한국어에서 형태소분석은 자연어처리를 위한 가장 기본적인 전처리 과정이므로 속도가 매우 중요한 요소라고 생각합니다. 따라서 자연어처리에 많이 사용하는 Long-Short Term Memory(LSTM)와 같은 Recurrent Neural Network(RNN) 알고리즘은 속도 면에서 활용도가 떨어질 것으로 예상하여 고려 대상에서 제외하였습니다.

## 성능

### 정확도

#### v0.3

CNN 모델의 주요 하이퍼 파라미터는 분류하려는 음절의 좌/우 문맥의 크기를 나타내는 win 값과, 음절 임베딩의 차원을 나타내는 emb 값입니다. win 값은 {2, 3, 4, 5, 7, 10}의 값을 가지며, emb 값은 {20, 30, 40, 50, 70, 100, 150, 200, 300, 500}의 값을 가집니다. 따라서 이 두 가지 값의 조합은 6 x 10으로 총 60가지를 실험하였고 아래와 같은 성능을 보였습니다. 성능 지표는 정확률과 재현율의 조화 평균값인 F-Score입니다.

win 파라미터의 경우 3 혹은 4에서 가장 좋은 성능을 보이며 그 이상에서는 오히려 성능이 떨어집니다. emb 파라미터의 경우 150까지는 성능도 같이 높아지다가 그 이상에서는 별 차이가 없습니다. 최 상위 5위 중 비교적 작은 모델은 win=3, emb=150으로 F-Score 값은 97.11입니다. 이 모델을 large 모델이라 명명합니다.

#### v0.4

[띄어쓰기 오류에 강건한 모델을 위한 실험](https://github.com/kakao/khaiii/wiki/띄어쓰기-오류에-강건한-모델을-위한-실험)을 통해 모델을 개선하였습니다. v0.4 모델은 띄어쓰기가 잘 되어있지 않은 입력에 대해 보다 좋은 성능을 보이는데 반해 세종 코퍼스에서는 다소 정확도가 떨어집니다. 이러한 점을 보완하기 위해 base 및 large 모델의 파라미터를 아래와 같이 조금 변경했습니다.

- base 모델: win=4, emb=35, F-Score: 94.96
- large 모델: win=4, emb=180, F-Score: 96.71

### 속도

#### v0.3

모델의 크기가 커지면 정확도가 높아지긴 하지만 그만큼 계산량 또한 많아져 속도가 떨어집니다. 그래서 적당한 정확도를 갖는 모델 중에서 크기가 작아 속도가 빠른 모델을 base 모델로 선정하였습니다. F-Score 값이 95 이상이면서 모델의 크기가 작은 모델은 win=3, emb=30이며 F-Score는 95.30입니다.

속도를 비교하기 위해 1만 문장(총 903KB, 문장 평균 91)의 텍스트를 분석해 비교했습니다. base 모델의 경우 약 10.5초, large 모델의 경우 약 78.8초가 걸립니다.

#### v0.4

모델의 크기가 커짐에 따라 아래와 같이 base, large 모델의 속도를 다시 측정했으며 v0.4 버전에서 다소 느려졌습니다.

- base 모델: 10.8 -> 14.4
- large 모델: 87.3 -> 165

## 사용자 사전

신경망 알고리즘은 소위 말하는 블랙박스 알고리즘으로 결과를 유추하는 과정을 사람이 따라가기가 쉽지 않습니다. 그래서 오분석이 발생할 경우 모델의 파라미터를 수정하여 바른 결과를 내도록 하는 것이 매우 어렵습니다. 이를 위해 khaiii에서는 신경망 알고리즘의 앞단에 기분석 사전을 뒷단에 오분석 패치라는 두 가지 사용자 사전 장치를 마련해 두었습니다.

### 기분석 사전

기분석 사전은 단일 어절에 대해 문맥에 상관없이 일괄적인 분석 결과를 갖는 경우에 사용합니다. 예를 들어 아래와 같은 엔트리가 있다면,

| 입력 어절 | 분석 결과    |
| --------- | ------------ |
| 이더리움* | 이더리움/NNP |

문장에서 `이더리움`으로 시작하는 모든 어절은 신경망 알고리즘을 사용하지 않고 `이더리움/NNP`로 동일하게 분석합니다.

세종 코퍼스에서 분석 모호성이 없는 어절들로부터 자동으로 기분석 사전을 추출할 경우 약 8만 개의 엔트리가 생성됩니다. 이를 적용할 경우 약간의 속도 향상도 있어서 base 모델에 적용하면 약 9.2초로 10% 정도 속도 향상이 있었습니다.

기분석 사전의 기술 방법 및 자세한 내용은 [기분석 사전 문서](https://github.com/kakao/khaiii/wiki/기분석-사전)를 참고하시기 바랍니다.

### 오분석 패치

오분석 패치는 여러 어절에 걸쳐서 충분한 문맥과 함께 오분석을 바로잡아야 할 경우에 사용합니다. 예를 들어 아래와 같은 엔트리가 있다면,

| 입력 텍스트 | 오분석 결과                             | 정분석 결과                                |
| ----------- | --------------------------------------- | ------------------------------------------ |
| 이 다른 것  | 이/JKS + _ + 다/VA + 른/MM + _ + 것/NNB | 이/JKS + _ + 다르/VA + ㄴ/ETM + _ + 것/NNB |

만약 khaiii가 위 "오분석 결과"와 같이 오분석을 발생한 경우에 한해 바른 분석 결과인 "정분석 결과"로 수정합니다. 여기서 "_"는 어절 간 경계, 즉 공백을 의미합니다.

오분석 패치의 기술 방법 및 자세한 내용은 [오분석 패치 문서](https://github.com/kakao/khaiii/wiki/오분석-패치)를 참고하시기 바랍니다.

**NNB NNP와 같은 단어의 뜻은 형태소 품사의 태그를 의미**

**CNN : 합성곱 신경망**

**khaiii는 데이터 기반으로 동작하기 때문에 기계학습 알고리즘(딥러닝)을 사용**

입력의 경우 각각의 음절이 분류 대상

문장을 띄어쓰기 단위로 토크나이징(토큰을 형태소단위로 쪼갬)한 뒤, 형태소와 품사를 붙인 형태로 결과 출력

**analyzed가 쪼개준 형태소 단위로 튜플을 만들고, 한 문장(혹은 구문)을 한 리스트로 합친다.**

**띄어쓰기에 취약(하지만 v.04에 따르면 띄어쓰기 오류에 대한 여러 모델을 실험하여 개선하였다고 합니다.)** 



## 오픈소스의 기능과, 이를 선정한 이유 ##

 카카오에서 개발한 한글의 형태소 분석 오픈소스인 khaiii는 문장을 띄어쓰기 단위로 토크나이징(토큰을 형태소단위로 쪼갬)한 뒤, 형태소와 품사를 붙인 형태로 결과 출력해주는 기능을 갖고 있으며, 이는 **analyzed가 쪼개준 형태소 단위로 튜플을 만들고, 한 문장(혹은 구문)을 한 리스트로 합쳐줍니다.** 또한, 신경망 알고리즘은 소위 말하는 블랙박스 알고리즘으로 결과를 유추하는 과정을 사람이 따라가기가 쉽지 않기 때문에 오분석이 발생할 경우 모델의 파라미터를 수정하여 바른 결과를 내도록 하는 것이 매우 어렵습니다. **이를 위해 khaiii에서는 신경망 알고리즘의 앞단에 기분석 사전을 뒷단에 오분석 패치라는 두 가지 사용자 사전 장치를 마련해 두었습니다. 입력의 경우 각각의 음절이 분류 대상입니다.**



**오픈소스 혹은 프로그램과 상호작용 초기 구상**

가공된 사용자의 검색 정보가 **django**로 온 시점부터 시작: 

가공한 정보가 도착하면 도착정보와 관련된 키워드를 해쉬태그와 매치,**sns**상에서 해쉬태그를 기반으로 추출한 자료들을**MaraiDB**에 전송, **MaraiDB**에서 보내 준 url을 통해 **scrapy**가 크롤링한 내용을 가져오고, 가져온 리스트안의 내용을 파이썬 한글 맞춤법 검사 라이브러리인 **Hanspell**을 이용하여 가공한 뒤에, 가공한 자료에서 **khaiii**를 이용하여 형태소 단위로 쪼개어 **MaraiDB**와 **scrapy**로 전송한 뒤, **MaraiDB**는 데이터베이스에 저장, **scrapy**는 분석한 키워드를 기반으로**place**를 크롤링한 후, 크롤링 한 내용을 **MaraiDB**로 전송 후, **django**에 자료 전송, 추가로 **elasticsearch**에도 쪼개진 키워드 제공가능, **elasticsearch**는 쪼개진 형태소로 색인어를 생성하고 색인어를 이용하여 빠르게 검색

처음 구상해봤던 다른 앱과의 상호작용

![img](file:///C:/Users/PC/AppData/Local/Temp/msohtmlclip1/01/clip_image002.png)



## khaiii로 구상해본 기능들

---

사전추가기능도있음
형태소를 음절단위로 쪼개서 전달
기계학습기반 알고리즘
형태소를 분석하여  비슷한 형태소조합을 학습시켜  데이터를 쌓아가고 유사어, 동음이의어에 대해 뒤에오는 조사,어미,접미사 를 미리 유추 할 수 있게하여 (파싱)보다 정확한 검색을 도와주는 것을 기대 (동음어는 앞 뒤 내용에 따라 뜻이 천차만별로 바뀌기 때문에 학습이 필요) 

형태소 분석은 자연어 처리의 가장 기초적인 절차로 이후 구문 분석이나 의미 분석으로 나아가기 위해 가장 먼저 이루어져야 하는 과정이라고 볼 수 있다
Konlpy가 gpl 라이선스라 충돌을 막기위해 아파치2 라이선스인 khaiii사용 

기분석사전,오분석패치 이용하여 정확도개선 

기분석사전
기분석 사전은 단일 어절에 대해 문맥에 상관없이 일괄적인 분석 결과를 갖는 경우 사용합니다. 

사전 엔트리의 종류 

기분석 사전의 엔트리는 아래와 같은 두 가지 종류가 있습니다. 

• 완전 일치: 전체 어절이 완전히 일치하는 경우에 적용되는 엔트리 

• 전방 매칭: 어절의 앞부분부터 부분적으로 일치할 경우에도 적용되는 엔트리 

더 긴 문장을 우선으로 분리

기분석 사전                                  
기계학습 모델 실행 전에 적용 
기계학습 모델의 결과에 적용
분석 속도를 빠르게 함
오분석 패치   
분석 속도가 느려짐
단일 어절에 한해 적용 가능
어절과 형태소 개수에 제한이 없음


데이터 -> 모델 -> 오분석 패치/기분석 사전 

make_vocab(vocab.in/vocab.out 생성) -> vocabulary -> morph, resource->sentence(공백 masked 음절 단위로 쪼개진 문장) -> dataset(음절 to tensor) -> embedder  -> models ->  trainer -> evaluator -> tagger-> char_align -> trie 

오분석패치
오분석된 내용을 정분석하여 추출

아파치2.0 

코드를 수정한 경우, 표시
Apache 로고, 이름 등 상표 사용에 제한을 두고 있음
작업에 Apache 라이선스를 적용하려면 대괄호 "[]"로 묶인 필드를 자신의 식별 정보로 대체하여 다음 상용구 고지를 첨부



형태소 분석을 하는 이유는 주로 형태소 단위로 의미있는 단어 를 가져가고 싶거나 품사 태깅을 통해 형용사나 명사를 추출하고 싶을 때 많이 이용하게 됨 

Khaiii는 python을 사용을 할 수 있게 했지만 내부는 C언어로 돌아감 - 속도 때문에



## 시행착오와, 해결 과정 ## 

다른 오픈소스들과 연계에 관해 어긋나는 부분이 보여, 형태소 분석활용의 이유에 대해 조사를 해보니 보통 형태소분석 활용을 많이 하는 부분이 색인어 추출인데 일라스틱서치랑도 연계해서 스크래피가 긁어온 내용을 크롤링해서 긁어오고 검색이들어오면 카이가 쪼갠 단어로 색인어를 추출해서 일라스틱서치에 반환하고 반환받은 색인어를 이용해서 색인을 만든 뒤에 일라스틱서치가 자체 인풋 플러그인을 이용해서 빠르게 검색해서 스크래피로 전달하면 그 내용을 마리아 db를 거쳐 장고가 문서화하는 식으로 dfd설계를 살짝 바꾸는 것이 더 자연스러워 보였습니다. 그리고 스크래피에서 카이를 거치는 부분은 빼고 일라스틱 색인어를 만들어주는 방향으로 가는걸로 하였습니다. 또한, 그 글의 내용과 해시태그를 서로 비교해서 해당 글이 얼마나 신뢰성이 높은가를 체크해주는 것을 기대하여 사람들이 조회수를 올리려고 해쉬태그를 달면 실제 글이랑 해쉬태그랑 비교해서 글의 신뢰성을 판단하고 신뢰성이 낮으면 데이터베이스에 넣지않고 높다면 넣는 과정에서 비교하려면 실제 글을 형태소 분석해서 단어단위로 쪼개줘야하므로 카이가 필요하긴 하다고 생각했었습니다. 마지막으로 이를 장고에서 문서화 해주도록 하였습니다.

# py-hanspell

[![Build Status](https://camo.githubusercontent.com/e267848bb52dc3afbdb308a8505a483b89d490cbc48848436f82c5c20c6549e4/68747470733a2f2f7472617669732d63692e6f72672f737375742f70792d68616e7370656c6c2e7376673f6272616e63683d6d6173746572)](https://travis-ci.org/ssut/py-hanspell) [![PyPI version](https://camo.githubusercontent.com/5feb0662b811d03c5ea15ed29231d7d036e9125af5def168490dca1f1ec69821/68747470733a2f2f62616467652e667572792e696f2f70792f70792d68616e7370656c6c2e737667)](http://badge.fury.io/py/py-hanspell)

py-hanspell은 네이버 맞춤법 검사기를 이용한 파이썬용 한글 맞춤법 검사 라이브러리입니다.

파이썬 2.7 및 3.4 모두 호환됩니다.

------

## 설치

설치하는 방법으로는 두 가지 방법이 있습니다.

우선 pip를 이용해 설치하는 방법이 있습니다. 커맨드 라인에 다음 명령어를 입력하시면 자동으로 설치가 진행됩니다

```
$ pip install py-hanspell
```

다음으로 이 GitHub 저장소에서 직접 내려받아 설치하는 방법입니다. 이 저장소를 로컬에 clone 하거나 우측에 보이는 메뉴에서 zip 파일로 다운받은 후에 로컬 커맨드 라인에

```
$ python setup.py install
```

를 입력하시거나, 또는 hanspell 폴더를 자신의 프로젝트 폴더 안에 포함시키면 됩니다.

### 필요한 라이브러리

- requests

## 사용 방법

```
>>> from hanspell import spell_checker
>>> result = spell_checker.check(u'안녕 하세요. 저는 한국인 입니다. 이문장은 한글로 작성됬습니다.')
>>> result.as_dict()  # dict로 출력
{'checked': '안녕하세요. 저는 한국인입니다. 이 문장은 한글로 작성됐습니다.',
 'errors': 4,
 'original': '안녕 하세요. 저는 한국인 입니다. 이문장은 한글로 작성됬습니다.',
 'result': True,
 'time': 0.07065701484680176,
 'words': {'안녕하세요.': 2,
           '저는': 0,
           '한국인입니다.': 2,
           '이': 2,
           '문장은': 2,
           '한글로': 0,
           '작성됐습니다.': 1}}
>>> result
Checked(result=True, original='안녕 하세요. 저는 한국인 입니다. 이문장은 한글로 작성됬습니다.', checked='안녕하세요. 저는 한국인입니다. 이 문장은 한글로 작성됐습니다.', errors=4, words=OrderedDict([('안녕하세요.', 2), ('저는', 0), ('한국인입니다.', 2), ('이', 2), ('문장은', 2), ('한글로', 0), ('작성됐습니다.', 1)]), time=0.10472893714904785)
```

### `list`로 주고받기

```
>>> from hanspell import spell_checker
>>> spell_checker.check([u'안녕 하세요.', u'저는 한국인 입니다.'])
[Checked(result=True, original='안녕 하세요.', checked='안녕하세요.', errors=1, words=OrderedDict([('안녕하세요.', 2)]), time=0.03297615051269531),
 Checked(result=True, original='저는 한국인 입니다.', checked='저는 한국인입니다.', errors=1, words=OrderedDict([('저는', 0), ('한국인입니다.', 2)]), time=0.029018878936767578)]
```

### Checked

| attribute | -                                                          |
| --------- | ---------------------------------------------------------- |
| result    | 맞춤법 검사 성공여부를 나타냅니다.                         |
| original  | 검사 전의 문장입니다.                                      |
| checked   | 맞춤법 검사 후의 문장입니다.                               |
| errors    | 맞춤법 오류 수를 나타냅니다.                               |
| words     | [Checked.words](https://github.com/ssut/py-hanspell#words) |
| time      | 총 요청 시간을 나타냅니다.                                 |

### Checked.words

위 사용 방법에 나와있는 words 부분은 교정된 최종 문장을 공백으로 나눈(split) 결과입니다.

결과는 key가 단어, value가 [CheckResult](https://github.com/ssut/py-hanspell#results)를 나타냅니다.

아래 코드를 참고하세요.

```
>>> for key, value in result.words.items():
...    print(key, value)
안녕하세요. 2
저는 0
한국인입니다. 2
이 2
문장은 2
한글로 0
작성됐습니다. 1
```

### CheckResult

아래 코드로 import 하신 후 비교에 사용할 수 있는 상수입니다.

```
from hanspell.constants import CheckResult
```

| .CONST                  | -                                           |
| ----------------------- | ------------------------------------------- |
| .PASSED                 | 맞춤법 검사 결과 문제가 없는 단어 또는 구절 |
| .WRONG_SPELLING         | 맞춤법에 문제가 있는 단어 또는 구절         |
| .WRONG_SPACING          | 띄어쓰기에 문제가 있는 단어 또는 구절       |
| .AMBIGUOUS              | 표준어가 의심되는 단어 또는 구절            |
| .STATISTICAL_CORRECTION | 통계적 교정에 따른 단어 또는 구절           |

## 라이브러리 사용에 대한 안내

이 라이브러리는 네이버 한글 맞춤법 검사기를 바탕으로 만들어진 라이브러리입니다.

모든 결과 및 데이터에 대한 저작권 및 책임은 네이버 주식회사에 있으며, 라이브러리를 상업적으로 사용하거나 불법적인 용도로 활용한 부분에 대해서는 그 어떠한 부분에 대해서도 개발자는 책임지지 않습니다.

## 변경내역

- **버전 1.1**: list 타입으로 주고받는 기능 지원, 처리속도 향상
- **버전 1.0**: 첫 버전 릴리즈

## 라이선스(License)

py-hanspell은 MIT License로 제공됩니다. 